[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "",
    "section": "",
    "text": "introduction\n\n\n\n    \n  \n    \n    \n    \n    \n    \n    \n    INTRODUCTION\n     Would you say that you have ever been a victim of crime? Can you think of someone who has been a victim of crime? How often do you think about safeguarding yourself and your possessions? You are probably still reading this because, even if you responded no to all of these questions, you are still interested in finding out how crime might be minimized.\n    The United States has one of the highest crime rates in the entire globe. Some offenders are imprisoned while others are free. There are laws and prisons in place to punish offenders, just like in any other nation. There is usually a large number of inmates housed in American prisons. For as long as there have been norms about how people should live and participate in society, crime has been an issue in America. Criminological trends have ebbed and flowed throughout history, much like ocean waves, complete with troughs and crests that can be easily identified. The incidence of crime tends to grow whenever something truly momentous happens in history. Most people in the United States believe crime rates have been rising over the past few decades. Criminologist public reports form the basis of this view. Crime rates and the efficiency of the laws that regulate our society are heavily measured using statistical data. National crime reports are the product of this data collection and analysis. Our societal legal framework has been improved thanks to these reports' implementation.\n    Criminal acts can be broken down into a few broad categories, including but not limited to crimes against persons, crimes against property, incipient crimes, statutory crimes, and financial crimes.\n    \n    Rape, robbery, arson, vehicle theft, etc., have all become disturbingly commonplace news items in the area. Communities in the United States are increasingly hospitable to various types of criminals. The government has spent a lot of money fighting crime, but the crime rate still seems high. As a result, crime has been an issue the American government has worked to resolve for many years. Nothing good can come from a government dominated by attorneys, especially given the state of corruption in law enforcement. So it may be argued that domestic terrorism is a bigger threat to the United States than international terrorism.\n    Anxiety and fear have increased as a result of the high crime rates in the United States. Families in the United States are living in fear, and residents are reluctant to venture out at night for fear of being raped or attacked by criminals. A lot of people are upset about this because they want the killings and rapes that have been happening at an alarming rate to stop. Drugs are a contributing factor to criminal activity, which has been a headache for law enforcement for decades. In practically every middle-class area, drugs are being marketed, and the most worrisome aspect of this is that drug peddlers are increasingly employing students as a means of reaching young consumers. Since many young people who try drugs for the first time become delinquent and are eventually introduced to the criminal world, this has contributed significantly to a rise in crime rates across the United States. Young people's participation in drug trafficking not only causes academic disruption in the event of arrest, but also makes them less competitive job candidates. It's for this reason that most drug-addicted youth end up in criminal careers. These realities have caused American parents to constantly worry about the safety of their children and to take preventative steps like keeping a careful check on their kids to ward off potential rapists, kidnappers, or drug traffickers.\n    Additionally, homicide is epidemic in American communities. According to estimates, a murder occurs every 30 minutes somewhere in the United States (Drehle, 2010, p. 1). In order to commit armed robberies, criminals have access to illegal weapons. Homicide rates in our communities have gone up alongside the number of firearms in criminal hands, since armed robbers are now more likely to take the lives of their victims if they have reason to believe that their victims may be able to provide useful information in police investigations. Property crimes like arson, car theft, etc., are also quite widespread in our area. Communities across the United States are also experiencing an increase in hate crimes. Disparities of race, sexual orientation, religion, etc., serve as their inspiration. Such crimes, most of which are motivated by sexual orientation, are unfortunately widespread in universities and colleges. Outside of the college campuses, racial tensions are the leading cause of hate crimes in the surrounding communities.\n    No one knows for certain that there is just one way to stop criminal activity, which complicates the task of developing and implementing effective crime prevention techniques. Some theorists believe a serious war on drugs could have a positive effect on the fight against crime, and police have long held the view that better police work could help bring crime rates down. Meanwhile, demographists and those advocating for strategic changes in a community's demographics have also argued that such measures would be helpful. Finally, supporters of the Clinton and Bush administrations have advocated for the hiring of additional police officers to address the problem. One economist, Steven Levitt, thinks that the legalization of abortion significantly lowered the number of violent criminals in the United States. His main point was that undesired children born in the 1980s grew up to commit crimes in the 1990s (Anderson, 2010, p. 1). All the above points have merit, but ultimately, it will take a mix of measures, including a reduction in prison populations, to win the war on crime. It's common knowledge that more offenders will be locked up if more prisons are built to relieve the issue of jail overcrowding. Incarcerating these offenders will improve their communities by making everyone there feel safer, especially those who are closest to them.\n    Since the subject of crime holds a great deal of fascination for me, I intend to use this investigation of the crime rate in the United States to answer the following questions:\n    ● Where in the United States may people expect the highest crime rates?\n    ● In what ways has crime increased recently? \n    ● What kinds of criminal activity can be expected in the various parts of the country?\n    ● How are crime location and crime types related?\n    ● Which part of the year do crime rate increase?\n    ● Which is the worst type of crime?\n    ● Who is targeted the most among men, women, and children?\n    ● Can we get a sense of what people believe about criminality??\n    ● Does race affect the rate of crime?\n    ● Which type of crime, person or society, is most impacted??"
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "",
    "section": "Data Source",
    "text": "Data Source"
  },
  {
    "objectID": "index.html#reddit",
    "href": "index.html#reddit",
    "title": "",
    "section": "Reddit",
    "text": "Reddit\nReddit is a widely used social media platform where users engage in discussions, share content, and provide insights on various topics. Each subreddit, which is a community centered around a specific topic, serves as a unique source of information and conversation. The data was retrived from Azure Blob Storage using PySpark. Since, our focus is on analyzing public responses to Taylor Swift, so we specifically targeted the r/TaylorSwift subreddit."
  },
  {
    "objectID": "index.html#spotify",
    "href": "index.html#spotify",
    "title": "",
    "section": "Spotify",
    "text": "Spotify\nSpotify is a prominent music streaming platform from which we accessed data via the Spotify Web API. Our primary focus was on the artist Taylor Swift, for which we specified “Taylor Swift” as the keyword in the artist field. This data source provides us with comprehensive insights into her music, including album details, track information, popularity metrics, and user-generated playlists."
  },
  {
    "objectID": "index.html#about-the-data",
    "href": "index.html#about-the-data",
    "title": "",
    "section": "About the data",
    "text": "About the data\nThe Reddit data comprises a substantial number of comments and discussions related to Taylor Swift across various subreddits. It includes details such as the username of the comment author, comment content, timestamps, upvote counts, and subreddit affiliations. This dataset is vast, containing a total of 1,974,887 rows, with various variables offering valuable insights into the online discourse surrounding Taylor Swift.\nOn the other hand, the Spotify data is a music-centric dataset that focuses on Taylor Swift’s musical catalog. It spans from 2006 to the latest available data in 2023, encompassing various attributes related to Taylor Swift’s songs and albums. This dataset includes essential music features such as danceability, energy, key, loudness, speechiness, and more. In total, there are 39 variables in the Spotify Dataset.\nWhile these datasets individually offer rich insights into Taylor Swift’s online presence and music characteristics, combining them allows for a holistic analysis. By merging these datasets, we can explore the relationship between online discussions, sentiments expressed in Reddit comments, and the musical features of Taylor Swift’s songs. This combined approach provides a comprehensive understanding of Taylor Swift’s impact on both the digital and musical landscapes, enabling us to derive meaningful insights and conclusions.\n\nReddit Data DescriptionSpotify Data Description\n\n\n\n\n\n\n\nTable 1: Description of Variables in Reddit Comment API Data\n\n\n\n\n\n\n\nTable 2: Description of Variables in Spotify Comment API Data"
  },
  {
    "objectID": "data_gathering.html",
    "href": "data_gathering.html",
    "title": "",
    "section": "",
    "text": "DATA GATHERING\n  Data gathering is the process of gathering and measuring information about variables of interest in a systematic way that allows one to answer stated research questions, test hypotheses, and evaluate results. All fields of study, such as the physical and social sciences, the humanities, business, etc., have to collect data as part of their research. Different fields use different methods, but the focus is always on making sure the data is accurate and honest.\n  The significance of ensuring precise and appropriate data gathering:\n  Regardless of the subject of study or preferred method of defining data (quantitative, qualitative), correct data collecting is necessary for preserving the validity of research. Errors are less likely to occur when proper data gathering instruments (existing, updated, or newly designed) and clear instructions for their right usage are chosen.\n  Consequences of incorrect data gathering include the following:\n  ● incapacity to appropriately answer research questions\n  ● failure to reproduce and validate the study\n  ● erroneous findings that result in wasted expenditure\n  ● tricking other researchers into investigating futile pathways\n  ● compromising public policy decisions\n  The two methods are:\n  ● Primary data gathering\n  This is original, first-hand data acquired by data researchers, as the name implies. This is the first phase in acquiring information, and it is completed before any more or related research is conducted. Primary data results are very accurate if the information is collected by the researcher. However, there is a drawback: first-hand research can be time-consuming and costly.\n  ● Secondary data gathering\n  Secondary data is data that has been gathered by third parties and has already been statistically analyzed. This material is either information that the researcher entrusted to others or information that the researcher searched up. Simply put, it's secondhand knowledge. Secondary information, while easier and less expensive to access than primary information, poses problems about veracity and validity. The majority of secondary data is quantitative.\n  \n  Types of Data:\n  TEXT DATA\n  Application Programming Interface (API)\n  One way in which programs can share information is through the use of an API, or application programming interface. You probably utilize an API every time you do anything on your phone, from sending a message to checking the score of a game. They both employ an application programming interface (API) to retrieve and send the data to your mobile device. An application programming interface (API) acts as an intermediary between a user and a service by receiving requests, translating them, and delivering responses. Developers essentially use APIs to gain access to resources on behalf of end users. Of course, an API only delivers the information that the application's developers have decided to make available. A valid API key is typically needed to authenticate a request made to an API. Instructions and prerequisites for using an API can be found in its accompanying documentation. Indeed, the utilization of several APIs is oftentimes entirely gratis. Following the existing API instructions, developers can typically construct a URL to get the data in a browser.\n\n  TWITTER API\n  A well-documented API, the Twitter API grants developers advanced access to the social media platform. You can use it to learn from and engage with Tweets. Direct messages, users, and other Twitter resources can all be interacted with. Developers can access user searches, block lists, real-time tweets, and more using Twitter's application programming interface (API).\n  Twitter has made a number of application programming interfaces (APIs) available to programmers. Researchers and businesses can use these APIs to mine Twitter for information. Smaller initiatives can also benefit from it, including data analysis, bot development, and even the creation of fully automated systems that can communicate with Twitter. The Twitter Application Programming Interface (API) will be used to retrieve recent public Tweets that are relevant to a given search query.\n  RECORD DATA\n  The record data is gathered from the FBI's website, Crime Data Explorer. The FBI's Crime Data Explorer (CDE) wants to make criminal and noncriminal law enforcement data sharing more open, easier to access, and better known. \n\n  About Record Data\n  There are several record data collected according to the requirements of analysis because crime is a broad topic with many subcategories. The reord data provides information on crime in the United States for various age groups, genders, and races. From 1960 to 2019, the state crime data provides information on crime types. The hate crime dataset provides information on offenders in the United States from 1990 to 2020.\n  \n\n\n  >\n        \n    \n      Snippet of Dataset\n        Description\n      \n      \n        \n        View \n         \n  \n        Download csv file\n        \n         R API FOR #CRIME\n           The API gives users broad access to public Twitter data that they have decided to share with the rest of the world. #CRIME used to gather data from Twitter in order to analyze people's perspectives on crime.\n          The data is extracted by querying Twitter for the hashtag and querying for the most recent tweets, which are then turned into dataframes based on author, time, text, and language.\n           \n           \n        R code \n        \n      \n      \n        \n        View \n         \n  \n        Download csv file\n        \n         R API FOR #FBI\n         The API gives users broad access to public Twitter data that they have decided to share with the rest of the world. #FBI used to gather data from Twitter in order to analyze people's perspectives on FBI.\n        The data is extracted by querying Twitter for the hashtag and querying for the most recent tweets, which are then turned into dataframes based on author, time, text, and language.\n          \n        \n           \n        R code \n        \n      \n      \n        \n        View \n         \n  \n        Download csv file\n        \n         R API FOR #CRIMEUSA\n         The API gives users broad access to public Twitter data that they have decided to share with the rest of the world. #CRIMEUSA used to gather data from Twitter in order to analyze people's perspectives on crime in USA.\n        The data is extracted by querying Twitter for the hashtag and querying for the most recent tweets, which are then turned into dataframes based on author, time, text, and language.\n          \n           \n        R code \n        \n      \n      \n        \n        View\n         \n  \n         download csv file \n         \n         Python API\n         Through the utilization of the Twitter API, tweets that have the hashtag \"crime\" in conjunction with a location are retrieved.\n        The data is extracted by querying Twitter for the hashtag and querying for the most recent tweets, which are then turned into dataframes based on author, time, text, and language.\n        \n        Python code \n        \n      \n      \n        \n        View \n         \n  \n        \n        Raw Data \n        Age Crime Data: Download csv file \n        Statewise Crime 1960-2019: Download csv file \n        Hate Crime 1991-2020: Download csv file \n        Crime Against Person by Location: Download csv file \n        Crime Against Person by Statewise: Download csv file \n        Crime Against Society by Location: Download csv file \n        Crime Against Society by Statewise: Download csv file \n        \n        The source of this raw data is collected from FBI crime data explorer."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "",
    "section": "",
    "text": "DATA CLEANING\n  Data cleaning is the process of deleting inaccurate, duplicate, or otherwise erroneous data from a dataset. These errors might include poorly structured data, duplicate entries, mislabeled data, and other difficulties; they frequently occur when two or more datasets are joined. Data cleansing enhances the quality of your data and any business decisions you make based on it.\n  There is no one correct approach to clean a dataset because each set is unique and contains its own collection of flaws that must be fixed. Many data cleaning processes can now be automated using specialist software, however some work must still be done manually to achieve maximum accuracy. This task is typically performed by data quality analysts, BI analysts, and business users.\n  Data Cleaning vs. Data Cleansing vs. Data Scrubbing\n   In the majority of instances, these names are interchangeable and refer to the same item. Data scrubbing may be used to refer to a particular component of data cleansing, specifically the elimination of redundant or inaccurate data from datasets. You should also be aware that data scrubbing might have a slightly different meaning within the context of data storage; in this case, it refers to an automated function that reviews storage systems and disk drives to identify and confirm the readability of any faulty sectors or blocks. Note that all three terms data cleaning, data cleansing, and data scrubbing are distinct from data transformation, which is the process of turning clean data to a new format or structure. Data transformation is a distinct procedure that follows data cleansing.\n  The majority of data cleansing steps adhere to a consistent framework:\n  ● Choose the relevant data values for your analysis.\n  ● Collect the information you require, then sort and organize \n  ● Identify and remove duplicate or irrelevant values.\n  ● Look for missing values and fill them in to get a complete dataset.\n  ● Fix any structural or repetitive errors left in the dataset.\n  ● Identify outliers and eliminate them so that they do not impede your analysis.\n  ● Validate your dataset to guarantee that it is suitable for data transformation and analysis.\n   TEXT DATA CLEANING:\n  ● Twitter databases contain more information, including retweet, hashtag, username, and altered tweets. This is entirely disregarded and eliminated from the dataset.\n  ● Generally, stop words are regarded as a \"one group of words.\" We do not want these terms to use database storage space. This is accomplished using NLTK and a \"Stop Word Dictionary.\" The stop words are eliminated since they serve no purpose.\n  ● According to the priorities, it is necessary to address all punctuation marks. For instance, \".\", \"!\",\"[\",\"?\" are essential punctuation marks that should be maintained, while others should be eliminated.\n  ● In the text datasets, additional information such as retweet, hashtag, username, and modified tweets are also included. This is all disregarded and removed from the dataset.\n  ● The duplicates are removed from the datasets. Occasionally, it is desirable to eliminate duplicate data using a collection of unique identifiers. For instance, the likelihood of two purchases occurring simultaneously with the identical square footage, price, and construction year is practically negligible.\n  RECORD DATA CLEANING:\n  After data is gathered, unnecessary columns are detected and eliminated. If two datasets include the same columns, they are combined.If null values are detected in the data, they are replaced with the column's mean. Some null values that contribute nothing to the data are eliminated. Checked for outliers and removed them to cleanse the data.\n  \n  \n  \n    \n      \n        Snippet of Clean Dataset\n        Description\n      \n      \n        \n            \n            View \n             \n\n            Download Clean CSV File\n            \n             Python Code - #CRIME Text Data\n            \n              Python is used to clean up Twitter text data. In order to conduct an analysis of #CRIME, all of the columns in the dataset were removed, and the one column that remained was the text column.  Tokenization from the NLTK library and Countvectorizer from the scikit-learn library are both used during this cleaning process. \n              \n            Python Code \n            \n        \n        \n      \n      \n      View \n       \n\n      Download Clean CSV File\n      Download Raw CSV File\n      \n       Python Code - #CRIMEUSA Text Data\n      Python is used to clean up Twitter text data. In order to conduct an analysis of #CRIMEUSA and #USACRIME, all of the columns in the dataset were removed, and the one column that remained was the text column.  Tokenization from the NLTK library and Countvectorizer from the scikit-learn library are both used during this cleaning process. \n      \n      Python Code \n      \n    \n    \n    \n      \n      View \n       \n\n      Download Clean CSV File\n      Download Raw CSV File\n      \n       Python Code - #FBI Text Data\n      Python is used to clean up Twitter text data. In order to conduct an analysis of #FBI, all of the columns in the dataset were removed, and the one column that remained was the text column.  Tokenization from the NLTK library and Countvectorizer from the scikit-learn library are both used during this cleaning process. \n      \n      Python Code \n      \n    \n    \n            \n            View \n             \n            Download Clean Statewise Crime 1960-2019 CSV File\n             \n             R Code - State Crime 1960-2019 Record Data\n            R is used to clean up data that has been labeled. Using R packages, unnecessary columns are removed and it is identified whether the dataset contains any NA values or duplicates. The information is used to find out where in the US Statewise crime of Violent crime and Property crime from 1960-2019. \n            \n            R Code \n            \n        \n    \n            \n            View \n             \n            Download Clean CSV File\n             \n             R Code - Age Crime Record Data\n            R is used for cleaning up labeled record data. The data is cleaned using R packages to eliminate unwanted columns and to determine whether the dataset contains any NA values or duplicates. The data are used to figure out how crimes in the USA vary by age group. \n            \n            R Code \n            \n        \n        \n            \n            View \n             \n            Download Clean Crime by Person US States CSV File\n            Download Clean Crime by Person US Location CSV File\n            Download Clean Crime by Society US States CSV File\n            Download Clean Crime by Society US Location CSV File\n             \n             R Code - Offenses in US Record Data\n            R is used to clean up data that has been labeled. Using R packages, unnecessary columns are removed and it is identified whether the dataset contains any NA values or duplicates. The information is used to find out where in the US crimes against people and crimes against society happen. \n            \n            R Code \n            \n        \n        \n            \n            View \n             \n            Download Clean Hate Crime 1991-2020 CSV File\n             \n             R Code - Hate Crime 1991-2020 Record Data\n            R is used to clean up data that has been labeled. Using R packages, unnecessary columns are removed and it is identified whether the dataset contains any NA values or duplicates. The information is used to find out where in the US Hate crimes from 1991-2020. \n            \n            R Code"
  },
  {
    "objectID": "exploring_data.html",
    "href": "exploring_data.html",
    "title": "",
    "section": "",
    "text": "EXPORING DATA\n           Exploring data is the visual depiction of data and information. Through the use of visual components such as charts, graphs, and maps, data visualization tools facilitate the identification and comprehension of trends, outliers, and patterns in data. In addition, it provides a good method for employees and business owners to deliver information to non-technical audiences without causing confusion.\n           Exploring data tools and technologies are vital in the realm of Big Data for analyzing enormous volumes of information and making data-driven decisions.\n           Providing information in a graphical manner may appear to have no disadvantages. But sometimes data might be misconstrued or misread when visualized using the incorrect technique. When deciding to develop a data visualization, it is best to consider both the benefits and drawbacks.\n           Advantage of Exploring data:\n           ● Quickly comprehend information:\n           Organisations may examine vast volumes of data in simple, unified ways by using graphical representations of business information - and derive inferences from that information. Furthermore, because examining information in graphical format is substantially faster than analyzing information in spreadsheets, firms may handle problems or answer inquiries in a more timely manner.\n           ● Recognize linkages and patterns:\n           When presented visually, even large volumes of complex data begin to make sense; organizations may identify parameters that are highly connected. Some of the connections will be clear, while others will not. Identifying such linkages allows companies to focus on areas that are most likely to have an impact on their most important goals.\n           ● Identify emerging trends:\n           Using data visualization to detect patterns - both in the business and in the market - can provide firms with a competitive advantage and eventually effect the bottom line. It is simple to identify outliers that affect product quality or customer attrition and address concerns before they become major issues.\n           ● Share the story with others.\n           Once a business has used visual analytics to find new insights, the next step is to share those insights with other people. In this step, it's important to use charts, graphs, or other visually striking ways to show data because it's interesting and gets the message across quickly.\n           Disadvantage of Exploring data:\n           ● Inaccurate or biased information.\n           ● Not always does correlation imply causation.\n           ● Messages of significance might be lost in translation.\n           Importance of Exploring data:\n           The significance of data exploration is straightforward: it enables individuals to see, interact with, and better comprehend data. Regardless of their degree of knowledge, the correct visualization can bring everyone on the same page, whether the information is basic or complex.\n           Setting the stage for visualizing data:\n           ● Know the size and number of rows and columns of the data you want to display (the uniqueness of data values in a column).\n           ● Figure out what you want to show and what kind of information you want to share.\n           ● Understand your audience and how they process visual information.\n           ● Utilize a visual that effectively and simply conveys the information to your audience.\n           Choosing which visual is the best:\n           One of the most difficult tasks for business users is determining which visual will best portray the information. SAS Visual Analytics utilizes intelligent autocharting to provide the optimal visual depending on the selected data.  When exploring a new data collection for the first time, autocharts are particularly beneficial because they provide a fast overview of vast volumes of data. This data exploration feature is beneficial for even seasoned statisticians seeking to accelerate the analytics lifecycle process, since it eliminates the need for recurrent sampling to establish which data is suitable for each model.\n           \n           \n           \n           \n           \n           Exploring Data with Python\n           Exploring Data with R\n           \n           \n            \n            \n            View \n            View Code and Visualization\n             \n            The word cloud of Twitter data reveals the most common context in which #crime is used. Words like \"investigate\" and \"homicide\" are also common, giving some idea of the context in which individuals are using the hashtag.  \n            \n            \n            \n            View \n            View Code and Visualization\n             \n             There are numerous crimes occurring around us for a variety of reasons, and the above bar graph depicts the distribution of crime against people and society. \n            \n            \n        \n        \n            \n            \n            View \n            View Code and Visualization\n             \n\n             As shown in the accompanying line graph, there is a positive correlation between age and the number of crimes; as age increases, so does the crime rate.  \n            \n            \n                \n                View \n                View Code and Visualization\n                 \n                The above bar graph compares the five states in the United States that have the highest rate of crime corresponding to their type of offense.  \n                \n            \n        \n        \n        \n            \n            \n            View \n            View Code and Visualization\n             \n\n             The graphs that are above tell us about the top 10 and least 10 crimes, and as we can see, drug abuse and property crimes are two of those that are in the top 10. However, robbery and gambling fall under the least 10 category, despite the fact that these are two crimes that are fairly common according to our knowledge, so we can imagine how significant the proportional impact of the top crimes will be. \n            \n            \n                \n                View \n                View Code and Visualization\n                 \n                 The crime rate for each category of crime, including crimes against people and crimes against society, is displayed in the line graph that can be found above.  \n                \n            \n            \n        \n        \n            \n            \n            View \n            View Code and Visualization\n             \n\n            he multiple bar graphs above show the most and least common offenses committed by people in each age category; as can be seen, all of these crimes disproportionately affect the elderly.  \n            \n            \n                \n                View \n                View Code and Visualization\n                 \n                The scatter plot indicates that the population and crime rate are positively correlated.  \n                \n            \n              \n          \n      \n        \n        \n        View \n              View Code and Visualization\n         \n        The above line graph demonstrates that the property crime rate is larger than the violent crime rate, but has been declining gradually since the late 1990s. \n        \n        \n        \n        View \n              View Code and Visualization\n         \n        The above donut pie chart shows that the rate of White's is higher than that of other races, although most of the races weren't known. This is followed by the rate of Black or African American people. \n        \n        \n      \n      \n        \n        \n        View \n              View Code and Visualization\n         \n        The density plot for property crime rate and violent crime rate indicates that motor crime is more prevalent in property crime, while assault is more prevalent in violent crime.  \n        \n        \n        \n        View \n              View Code and Visualization\n         \n        The above bar chart shows that December has the lowest crime rate in the United States, while September and October have the highest."
  },
  {
    "objectID": "nbrd.html",
    "href": "nbrd.html",
    "title": "",
    "section": "",
    "text": "NAIVE BAYES FOR RECORD DATA\n  \n    \n    NAIVE BAYES FOR RECORD DATA \n    INTRODUCTION \n    Naive Bayes is a classification method based on Bayes' Theorem and the assumption of predictor independence. A Naive Bayes classifier posits, in simple terms, that the existence of one feature in a class is independent to the presence of other features.\n     The Naive Bayes model is simple to construct and especially suitable for extremely big data sets. In addition to its simplicity, Naive Bayes is known to outperform even the most complex classification techniques.\n    Assumption for Naive Bayes: \n     The Naive Bayes algorithm makes the assumption that the predictors are independent of each other. In real life, it is almost impossible that we get a set of predictors which are completely independent. \n    Types of Naive Bayes Classifier:\n     ● Multinomial Naive Bayes:\n     This is mostly used to determine how to place a document into a certain category, such as sports, politics, technology, etc. The frequency of the words in the document is one of the things that the classifier looks at.\n    ● Bernoulli Naive Bayes:\n     This is comparable to the multinomial naive bayes model, with the exception that the predictors are boolean variables. The parameters that we use to predict the class variable take on simply yes or no values, such as whether or not a word appears in the text.\n    ● Gaussian Naive Bayes:\n     When predictors have continuous values and are not discrete, these values are assumed to be drawn from a Gaussian distribution.\n    Advantages of Naive Bayes\n    ● This algorithm is efficient and can save a great deal of time.\n    ● The Naive Bayes method can be used to solve multiclass prediction issues.\n    ● If the model's assumption on the independence of characteristics remains true, it can outperform competing models and requires significantly less training data.\n    ● The Naive Bayes algorithm is more suitable for category input variables than numerical ones.\n    Disadvantages of Naive Bayes\n    ● Naive Bayes assumes that all predictors (or features) are independent, which rarely happens in real life. This makes it harder for this algorithm to be used in the real world.\n    ● The \"zero-frequency problem\" is when this algorithm gives a probability of 0 to a categorical variable whose category in the test data set wasn't in the training data set. The best way to solve this problem would be to use a smoothing method.\n    ● Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously. \n                \n    ABOUT THE DATA\n     The record data utilized by Naive Bayes pertains to the Homicide (murder and non-negligent manslaughter), rape, robbery, and serious assault constitute violent crime. Violent crimes involve the use or threat of physical force.\n     The data on the Crime Data Explorer covers just crimes that have been reported; it is not a full record of all crimes. Before assessing the data, it is crucial to understand the many causes of criminal activity and crime reporting in a community. Without these factors, existing data can be misleading. There are many things to think about, like the size and density of the population, the economy, the unemployment rate, the prosecutorial, judicial, and correctional policies, the administrative and investigative focus of law enforcement, how people feel about crime and police, and how strong the police force really is.  \n     The dataset contains three types of data about violent crime victims: their age, gender, and race, with different type of crime offenses.\n     The state crime rate data contains crime rate for property and violent crime category. For the naive bayes, the violent crime rate is considered which is further categoried into differnt type of violent crime such as murder, rape, robbery and assault crime. The other set of dataset is the hate crime data from the year 1990-2020.\n    \n    DATA CLEANING AND VISUALISATION\n     Code for cleaning and Visulation of the data here \n    R is utilized in the process of cleaning up data that has previously been labeled. By utilizing R packages, it is possible to eliminate columns that are not necessary and determine whether or not the dataset contains any NA values or duplicates.\n    \n      \n   \n    \n      \n        \n          \n          \n          View \n                Victim Yearly Crime Rate CSV file\n           \n          \n    \n          \n          \n          View \n                View Code and Visualization\n           \n          There is variation in the violent crime rate, as shown in the graph the crime rate has been incresed since 2018.\n            \n        \n            \n          \n        \n          \n          \n          View \n                Victim Age CSV file\n           \n         \n          \n    \n          \n          \n          View \n                View Code and Visualization\n           \n          The age distribution of victim's is high between age 20-35 and the children are not mostly the victims when compared to adults.\n         \n            \n        \n            \n          \n          \n          \n            \n            \n            View \n                  Victim Gender CSV file\n             \n            \n      \n            \n            \n            View \n                  View Code and Visualization\n             \n            As shown in the piechart, females are mostly the victims when comapred to males.\n              \n          \n              \n            \n\n            \n              \n              \n              View \n                    Victim Race CSV file\n               \n              \n        \n              \n              \n              View \n                    View Code and Visualization\n               \n              White's are  mostly the victims when comapred to other race, followed by Black's or African American's.\n            \n                \n            \n                \n              \n\n              \n                \n                \n                View \n                      State Crime Rate 1960-2019 CSV file\n                 \n               \n                \n          \n                \n                \n                View \n                      View Code and Visualization\n                 \n                The line graph demonstrates that the property crime rate is larger than the violent crime rate, but has been declining gradually since the late 1990s.\n               \n                  \n              \n                  \n                \n              \n                \n                  \n                  \n                  View \n                        Hate Crime Rate 1991-2020 CSV file\n                   \n                 \n                  \n            \n                  \n                  \n                  View \n                        View Code and Visualization\n                   \n                  The donut pie chart shows that the rate of White's is higher than that of other races, although most of the races weren't known. This is followed by the rate of Black or African American people.\n                 \n                    \n                \n                    \n                  \n      \n    \n\n    \n    \n    MODEL BUILDING\n    The dataset is divided into training and testing sets before the model is built. The split ratio in the training set is 0.75 and in the testing set it is 0.25. The training dataset is used to train the naive bayes model, which is then tested using labels from the testing dataset.\n    \n    \n     Model Building Code for Victim Age Data here \n    \n    \n      \n        \n          \n          \n          View \n             \n          \n    \n          \n          \n          View \n           \n        \n            \n        \n            \n          \n            \n            View \n             \n           \n              \n          \n          \n      \n    \n    \n     Model Building Code for Victim Gender Data here \n  \n    \n      \n        \n          \n          \n          View \n           \n          \n            \n        \n        \n          \n          View \n             \n          \n            \n          \n            \n            View \n             \n             \n              \n          \n          \n          \n\n        \n      \n      \n     Model Building Code for Victim Race Data here \n  \n    \n      \n        \n        \n        \n        View \n           \n        \n    \n        \n        \n        View \n         \n         \n            \n        \n            \n          \n            \n            View \n             \n            \n              \n          \n\n          \n            \n            View \n             \n             The graph illustrating the model's key features. \n              \n          \n          \n          \n          \n          \n           Model Building Code for Violent Crime Data here \n          \n          \n            \n              \n                \n                \n                View \n                 \n                \n                  \n              \n              \n                \n                View \n                   \n                \n                  \n                \n                  \n                  View \n                   \n                   \n                    \n                \n                \n                \n      \n              \n            \n            \n             Model Building Code for Hate Crime Data here \n          \n            \n              \n                \n                  \n                  \n                  View \n                   \n                  \n                    \n                \n                \n                  \n                  View \n                     \n                  \n                    \n                  \n                    \n                    View \n                     \n                     \n                      \n                  \n                  \n                  \n        \n                \n              \n      \n    \n\n    \n    \n    CONCLUSION\n    Given the types of crimes committed, the goal was to carry out a naive Bayes analysis so as to improve the accuracy of victim age, victim race, victim gender, violent crime rate and hate crime predictions for each category. The accuracy of the model in predicting the age group and hate crime is 60%, which is above average accuracy for that age group. The accuracy of the model in predicting gender and race is 75%, which is relatively reasonable accuracy for that gender and race group whereas the accuracy of the model in predicting violent crime from 1960-2019 is 93% which is relatively high."
  },
  {
    "objectID": "nbtd.html",
    "href": "nbtd.html",
    "title": "",
    "section": "",
    "text": "NAIVE BAYES FOR TEXT DATA\n  \n    \n    NAIVE BAYES FOR TEXT DATA \n    INTRODUCTION \n    Naive Bayes is a classification method based on Bayes' Theorem and the assumption of predictor independence. A Naive Bayes classifier posits, in simple terms, that the existence of one feature in a class is independent to the presence of other features.\n     The Naive Bayes model is simple to construct and especially suitable for extremely big data sets. In addition to its simplicity, Naive Bayes is known to outperform even the most complex classification techniques.\n    Assumption for Naive Bayes: \n     The Naive Bayes algorithm makes the assumption that the predictors are independent of each other. In real life, it is almost impossible that we get a set of predictors which are completely independent. \n    Types of Naive Bayes Classifier:\n     ● Multinomial Naive Bayes:\n     This is mostly used to determine how to place a document into a certain category, such as sports, politics, technology, etc. The frequency of the words in the document is one of the things that the classifier looks at.\n    ● Bernoulli Naive Bayes:\n     This is comparable to the multinomial naive bayes model, with the exception that the predictors are boolean variables. The parameters that we use to predict the class variable take on simply yes or no values, such as whether or not a word appears in the text.\n    ● Gaussian Naive Bayes:\n     When predictors have continuous values and are not discrete, these values are assumed to be drawn from a Gaussian distribution.\n    Advantages of Naive Bayes\n    ● This algorithm is efficient and can save a great deal of time.\n    ● The Naive Bayes method can be used to solve multiclass prediction issues.\n    ● If the model's assumption on the independence of characteristics remains true, it can outperform competing models and requires significantly less training data.\n    ● The Naive Bayes algorithm is more suitable for category input variables than numerical ones.\n    Disadvantages of Naive Bayes\n    ● Naive Bayes assumes that all predictors (or features) are independent, which rarely happens in real life. This makes it harder for this algorithm to be used in the real world.\n    ● The \"zero-frequency problem\" is when this algorithm gives a probability of 0 to a categorical variable whose category in the test data set wasn't in the training data set. The best way to solve this problem would be to use a smoothing method.\n    ● Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously. \n                \n\n    ABOUT THE DATA\n     The label text data is gathered from the #FBI and #USACRIME Twitter APIs. The #USACRIME and #CRIMEUSA hashtags are used interchangeably for the collecting of data regarding the use of hostages on Twitter, as both have the same purpose.The purpose of gathering this text data was to determine the public's view of the FBI and crime-related tweets in the United States.\n    \n    DATA CLEANING AND VISUALISATION\n    Python is used to clean up textual data from Twitter. Only the text column remained in the dataset after all other columns were eliminated in order to analyze #crime. During this cleaning procedure, both Tokenization from the NLTK library and Countvectorizer from the scikit-learn library are utilized.\n    To provide a concise summary of the data, a wordcloud of both hashtags has been created.\n    \n    \n   \n    \n      \n        \n          \n          \n          View \n                FBI CSV file\n           \n          \n          \n            \n            View \n            Data Cleaning and Visualization HTML File \n             \n            \n\n        \n    \n        \n            \n                \n                View \n                      USA Crime CSV file\n                 \n            \n            \n            \n              \n              View \n              Data Cleaning and Visualization HTML File \n               \n            \n              \n  \n            \n\n        \n        \n        \n        MODEL BUILDING\n        The code can be found here here \n        Before constructing a model, the dataset is divided into training and testing sets. The split ratio is 0.75 of the total number of data in the training set and 0.25 of the total number of data in the testing set. There are three distinct naive Bayes models built. The Naive Bayes models vary depending to the hypertuning of various parameters. primarily alpha values\n        Naive Bayes Model 1\n         The first model is built with the default parameters. The default parameters are alpha = 1.0 and fit_prior = True. The model is built with the training set and the accuracy of the model is calculated with the testing set. The accuracy of the model is 0.75. \n        The snapshot of the accuracy/heatmap is attached below.\n        \n        Naive Bayes Model 2\n        This is the second naive bayes model. In this model, the hyperparameter are alpha = 5. In this model, the accuracy is 95%. \n        The snapshot of the accuracy/heatmap is attached below.\n        \n        Naive Bayes Model 3\n        This is the third naive bayes model. In this model, the hyperparameter are alpha = 0. In this model, the accuracy is 94%.\n        The snapshot of the accuracy/heatmap is attached below.\n        \n        \n        CONCLUSION\n        The Naive Bayes model classified tweets generated from Twitter into the hashtag class (FBI and USACRIME) of different tweets. The accuracy of the models are 96% and 95%. The accuracy is pretty high. With the collection of words, the model is able to predict or classify the tweets into particular classes."
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "ARM AND NETWORKING FOR RECORD DATA",
    "section": "",
    "text": "INTRODUCTION\nThe primary objective is to find a quick way to summarize state record data from 1990 to 2019 using Networks. Networks provide a theoretical framework for conceptually representing interrelations in a wide variety of systems and discovering statistically significant interrelationships between variables in large databases.\nABOUT THE DATA\nThe record data is obtained from the official FBI website, which provides data on crime rates from 1990 to 2019 by year. This data includes two significant crime categories, property crime and violent crime, which are further subdivided into distinct types of crime.\nIMPORT LIBRARIES\n\n\nCode\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom apyori import apriori\nimport networkx as nx \nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nIMPORT DATA\n\n\nCode\ndf = pd.read_csv('../../data/modified-data/cleaned_state_crime_record_data.csv')\n#create new column with the predicted category\ndf['property_crime'] = df[['Property_Burglary_Rate', 'Property_Larceny_Rate','Property_Larceny_Rate']].idxmax(axis=1)\ndf['Violent_label'] = df[['Violent_Assault_Rate', 'Violent_Murder_Rate','Violent_Rape_Rate','Violent_Robbery_Rate']].idxmax(axis=1)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      State\n      Year\n      Population\n      Property_Crime_Rate\n      Property_Burglary_Rate\n      Property_Larceny_Rate\n      Property_Motor_Rate\n      Violent_Crime_Rate\n      Violent_Assault_Rate\n      Violent_Murder_Rate\n      Violent_Rape_Rate\n      Violent_Robbery_Rate\n      property_crime\n      Violent_label\n    \n  \n  \n    \n      0\n      Alabama\n      1960\n      3266740\n      1035.4\n      355.9\n      592.1\n      87.3\n      186.6\n      138.1\n      12.4\n      8.6\n      27.5\n      Property_Larceny_Rate\n      Violent_Assault_Rate\n    \n    \n      1\n      Alabama\n      1961\n      3302000\n      985.5\n      339.3\n      569.4\n      76.8\n      168.5\n      128.9\n      12.9\n      7.6\n      19.1\n      Property_Larceny_Rate\n      Violent_Assault_Rate\n    \n    \n      2\n      Alabama\n      1962\n      3358000\n      1067.0\n      349.1\n      634.5\n      83.4\n      157.3\n      119.0\n      9.4\n      6.5\n      22.5\n      Property_Larceny_Rate\n      Violent_Assault_Rate\n    \n    \n      3\n      Alabama\n      1963\n      3347000\n      1150.9\n      376.9\n      683.4\n      90.6\n      182.7\n      142.1\n      10.2\n      5.7\n      24.7\n      Property_Larceny_Rate\n      Violent_Assault_Rate\n    \n    \n      4\n      Alabama\n      1964\n      3407000\n      1358.7\n      466.6\n      784.1\n      108.0\n      213.1\n      163.0\n      9.3\n      11.7\n      29.1\n      Property_Larceny_Rate\n      Violent_Assault_Rate\n    \n  \n\n\n\n\nNETWORKING ANALYSIS\nA network is a structure that represents a group of objects/people and their relationships. In arithmetic, it is also known as a graph. The components of a network structure are nodes and edges. Here, nodes indicate the things that will be analyzed, while edges represent their relationships.\nNetwork Analysis has many practical applications in the real world. It facilitates a thorough comprehension of the structure of a relationship in social networks, the structure or process of change in natural phenomena, and the analysis of biological systems.\nIdentifying the essential node in a network is a crucial application of network analysis. This endeavor is known as Network Centrality Measurement.\nAdvantages of Network Analysis:\n\nNetwork analysis is a very important and powerful tool that is used to plan, schedule, and keep track of operations in large and complicated projects.\nNetwork analysis is a very useful tool for figuring out how well things are going compared to what was planned.\nUsing technology, network analysis can figure out how different tasks depend on each other so that they can be properly integrated and coordinated.\nNetwork analysis makes sure that the different parts of a project work well together and talk to each other.\nNetwork analysis looks at the trade-off between time and money and gives the best schedule for the project.\n\nDisadvantages of Network Analysis:\n\nIn network analysis, building networks for complex projects is hard and takes a lot of time.\nIt’s hard to guess how long different things will take in real time.\nAnalysis of the project is a very hard task because the project has a limited amount of resources.\n\nASSOCIATION RULE MINING\nAssociation rule mining is a very important data mining technique. It is used to find patterns in the database that show up often. The main goal of association rule mining is to find interesting connections and links between the different things in the database. Association rules are used in a lot of different areas to find patterns in the data. With patterns, we can figure out how many different ways events can happen at the same time. A lot of data is put through rules of association.\nIn association rule mining, rules are made through the following two steps:\n\nAll of the common item sets can be found with the least amount of help.\nStrong association rules are made from these sets of frequently used items, with confidence c above a predetermined threshold value.\n\nThe first step takes more concentration because it’s hard to find all of the often-used item sets.\nApriori Algorithm\nThe Apriori algorithm is the most common and well-known way to find association rules. It is used to make sets of items that are used often for the database. The main goal of the apriori algorithm is to figure out how different objects are related to each other. The association rule says how two or more objects are related to each other. Frequent pattern mining is another name for the Apriori algorithm. Most of the time, the Apriori algorithm is used on a database that has a lot of transactions.\nAdvantages of Apriori Algorithm:\n\nThis association rule learning algorithm is the simplest and easiest to understand.\nThe rules that come out of this are easy to understand and communicate to end users.\nIt doesn’t need labeled data because it is fully unsupervised. Because unlabeled data is often easier to get, you can use it in many different situations.\nBased on this implementation, many extensions for different uses were suggested. For example, there are association learning algorithms that take into account the order of items, their number, and the timestamps that go with them.\nThe algorithm is complete, so it finds all the rules that have the support and confidence levels that were given.\n\nDisadvantages of Apriori Algorithm:\n\nThe Apriori algorithm only needs a small amount of support in the data set.\nThe amount of time it takes to hold a lot of candidate sets with a lot of common item sets. So, it doesn’t work well when there are a lot of datasets.\n\n\n\nCode\nrecords = []\nfor i in range(len(df)):\n    records.append([str(df.values[i,j]) for j in range(0, 14)])\n\n\nRunning the apriori algorithm for min_support=0.003, min_confidence=0.2, min_lift=2, min_length=2 :\n\n\nCode\nassociation_rules = apriori(records, min_support=0.003, min_confidence=0.2, min_lift=2, min_length=2)\nassociation_results = list(association_rules)\n\n\nRESULTS\n\n\nCode\nfor item in association_results:\n    \n    # first index of the inner list\n    # Contains base item and add item\n    pair = item[0] \n    items = [x for x in pair]\n    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n    \n    #second index of the inner list\n    print(\"Support: \" + str(item[1]))\n    \n    #third index of the list located at 0th\n    #of the third index of the inner list\n\n    print(\"Confidence: \" + str(item[2][0][2]))\n    print(\"Lift: \" + str(item[2][0][3]))\n    print(\"=====================================\")\n\n\nRule: Violent_Robbery_Rate -> 1960\nSupport: 0.004815409309791332\nConfidence: 0.29411764705882354\nLift: 3.192252510760402\n=====================================\nRule: Violent_Robbery_Rate -> 1961\nSupport: 0.0038523274478330658\nConfidence: 0.23529411764705882\nLift: 2.5538020086083213\n=====================================\nRule: 1962 -> Violent_Robbery_Rate\nSupport: 0.003531300160513644\nConfidence: 0.21568627450980393\nLift: 2.3409851745576282\n=====================================\nRule: Violent_Robbery_Rate -> 1968\nSupport: 0.0038523274478330658\nConfidence: 0.23076923076923075\nLift: 2.5046904315196996\n=====================================\nRule: Violent_Robbery_Rate -> 1969\nSupport: 0.0044943820224719105\nConfidence: 0.2692307692307692\nLift: 2.9221388367729833\n=====================================\nRule: 1970 -> Violent_Robbery_Rate\nSupport: 0.004815409309791332\nConfidence: 0.28846153846153844\nLift: 3.1308630393996246\n=====================================\nRule: Violent_Robbery_Rate -> 1971\nSupport: 0.0044943820224719105\nConfidence: 0.2692307692307692\nLift: 2.9221388367729833\n=====================================\nRule: Violent_Robbery_Rate -> 1972\nSupport: 0.0038523274478330658\nConfidence: 0.23076923076923075\nLift: 2.5046904315196996\n=====================================\nRule: Violent_Robbery_Rate -> 1973\nSupport: 0.004173354735152488\nConfidence: 0.25\nLift: 2.7134146341463414\n=====================================\nRule: 1974 -> Violent_Robbery_Rate\nSupport: 0.0044943820224719105\nConfidence: 0.2692307692307692\nLift: 2.9221388367729833\n=====================================\nRule: Violent_Robbery_Rate -> 1975\nSupport: 0.004815409309791332\nConfidence: 0.28846153846153844\nLift: 3.1308630393996246\n=====================================\nRule: Wisconsin -> 2.8\nSupport: 0.0032102728731942215\nConfidence: 0.2564102564102564\nLift: 13.31196581196581\n=====================================\nRule: 5.2 -> Violent_Robbery_Rate\nSupport: 0.003531300160513644\nConfidence: 0.275\nLift: 2.984756097560976\n=====================================\nRule: District of Columbia -> Violent_Robbery_Rate\nSupport: 0.01059390048154093\nConfidence: 0.55\nLift: 5.969512195121952\n=====================================\nRule: Hawaii -> Violent_Robbery_Rate\nSupport: 0.006420545746388443\nConfidence: 0.33333333333333337\nLift: 3.6178861788617893\n=====================================\nRule: Violent_Robbery_Rate -> Illinois\nSupport: 0.005136436597110754\nConfidence: 0.26666666666666666\nLift: 2.894308943089431\n=====================================\nRule: Violent_Robbery_Rate -> Minnesota\nSupport: 0.006420545746388443\nConfidence: 0.33333333333333337\nLift: 3.6178861788617893\n=====================================\nRule: Nevada -> Violent_Robbery_Rate\nSupport: 0.006420545746388443\nConfidence: 0.33333333333333337\nLift: 3.6178861788617893\n=====================================\nRule: Violent_Robbery_Rate -> New Jersey\nSupport: 0.005457463884430177\nConfidence: 0.2833333333333334\nLift: 3.075203252032521\n=====================================\nRule: New York -> Violent_Robbery_Rate\nSupport: 0.009630818619582664\nConfidence: 0.5454545454545454\nLift: 5.9201773835920175\n=====================================\nRule: Violent_Robbery_Rate -> Ohio\nSupport: 0.009309791332263243\nConfidence: 0.4833333333333334\nLift: 5.2459349593495945\n=====================================\nRule: Violent_Robbery_Rate -> Pennsylvania\nSupport: 0.004815409309791332\nConfidence: 0.25\nLift: 2.7134146341463414\n=====================================\nRule: Violent_Robbery_Rate -> Property_Larceny_Rate\nSupport: 0.004815409309791332\nConfidence: 0.29411764705882354\nLift: 3.192252510760402\n=====================================\nRule: Violent_Robbery_Rate -> Property_Larceny_Rate\nSupport: 0.0038523274478330658\nConfidence: 0.23529411764705882\nLift: 2.5538020086083213\n=====================================\nRule: 1962 -> Violent_Robbery_Rate\nSupport: 0.003531300160513644\nConfidence: 0.21568627450980393\nLift: 2.3409851745576282\n=====================================\nRule: Violent_Robbery_Rate -> 1968\nSupport: 0.0038523274478330658\nConfidence: 0.23076923076923075\nLift: 2.5046904315196996\n=====================================\nRule: Violent_Robbery_Rate -> Property_Larceny_Rate\nSupport: 0.0044943820224719105\nConfidence: 0.2692307692307692\nLift: 2.9221388367729833\n=====================================\nRule: 1970 -> Property_Larceny_Rate\nSupport: 0.004815409309791332\nConfidence: 0.28846153846153844\nLift: 3.1308630393996246\n=====================================\nRule: Violent_Robbery_Rate -> Property_Larceny_Rate\nSupport: 0.0044943820224719105\nConfidence: 0.2692307692307692\nLift: 2.9221388367729833\n=====================================\nRule: Violent_Robbery_Rate -> Property_Larceny_Rate\nSupport: 0.0038523274478330658\nConfidence: 0.23076923076923075\nLift: 2.5046904315196996\n=====================================\nRule: Violent_Robbery_Rate -> Property_Larceny_Rate\nSupport: 0.004173354735152488\nConfidence: 0.25\nLift: 2.7134146341463414\n=====================================\nRule: 1974 -> Violent_Robbery_Rate\nSupport: 0.0044943820224719105\nConfidence: 0.2692307692307692\nLift: 2.9221388367729833\n=====================================\nRule: Violent_Robbery_Rate -> 1975\nSupport: 0.004815409309791332\nConfidence: 0.28846153846153844\nLift: 3.1308630393996246\n=====================================\nRule: Wisconsin -> Property_Larceny_Rate\nSupport: 0.0032102728731942215\nConfidence: 0.2564102564102564\nLift: 13.31196581196581\n=====================================\nRule: Violent_Assault_Rate -> Wisconsin\nSupport: 0.0032102728731942215\nConfidence: 0.2564102564102564\nLift: 13.7709991158267\n=====================================\nRule: 5.2 -> Violent_Robbery_Rate\nSupport: 0.003531300160513644\nConfidence: 0.275\nLift: 2.984756097560976\n=====================================\nRule: District of Columbia -> Property_Larceny_Rate\nSupport: 0.01059390048154093\nConfidence: 0.55\nLift: 5.969512195121952\n=====================================\nRule: Hawaii -> Violent_Robbery_Rate\nSupport: 0.006420545746388443\nConfidence: 0.33333333333333337\nLift: 3.6178861788617893\n=====================================\nRule: Violent_Robbery_Rate -> Illinois\nSupport: 0.005136436597110754\nConfidence: 0.26666666666666666\nLift: 2.894308943089431\n=====================================\nRule: Violent_Robbery_Rate -> Property_Larceny_Rate\nSupport: 0.006420545746388443\nConfidence: 0.33333333333333337\nLift: 3.6178861788617893\n=====================================\nRule: Nevada -> Violent_Robbery_Rate\nSupport: 0.006420545746388443\nConfidence: 0.33333333333333337\nLift: 3.6178861788617893\n=====================================\nRule: Violent_Robbery_Rate -> New Jersey\nSupport: 0.005457463884430177\nConfidence: 0.2833333333333334\nLift: 3.075203252032521\n=====================================\nRule: New York -> Violent_Robbery_Rate\nSupport: 0.009630818619582664\nConfidence: 0.5454545454545454\nLift: 5.9201773835920175\n=====================================\nRule: Violent_Robbery_Rate -> Property_Larceny_Rate\nSupport: 0.009309791332263243\nConfidence: 0.4833333333333334\nLift: 5.2459349593495945\n=====================================\nRule: Violent_Robbery_Rate -> Pennsylvania\nSupport: 0.004815409309791332\nConfidence: 0.25\nLift: 2.7134146341463414\n=====================================\nRule: Violent_Assault_Rate -> Wisconsin\nSupport: 0.0032102728731942215\nConfidence: 0.2564102564102564\nLift: 13.7709991158267\n=====================================\n\n\nHELPER FUNCTION - reformat_results: Reformats the results from the apriori algorithm into a dataframe with values for itemsets, support, confidence, and lift. - convert_to_network: Converts the apriori dataframe into a Network Graph. - plot_network: Visualisation of the Network Graph.\n\n\nCode\ndef reformat_results(results):\n\n    #CLEAN-UP RESULTS \n    keep=[]\n    for i in range(0,len(results)):\n        for j in range(0,len(list(results[i]))):\n            # print(results)\n            if(j>1):\n                for k in range(0,len(list(results[i][j]))):\n                    if(len(results[i][j][k][0])!=0):\n                        #print(len(results[i][j][k][0]),results[i][j][k][0])\n                        rhs=list(results[i][j][k][0])\n                        lhs=list(results[i][j][k][1])\n                        conf=float(results[i][j][k][2])\n                        lift=float(results[i][j][k][3])\n                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n                        # keep.append()\n            if(j==1):\n                supp=results[i][j]\n\n    return pd.DataFrame(keep, columns =[\"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"])\n\n\n\n\nCode\ndef convert_to_network(df):\n    #print(df)\n\n    #BUILD GRAPH\n    G = nx.DiGraph()  # DIRECTED\n    for row in df.iterrows():\n        # for column in df.columns:\n        lhs=\"_\".join(row[1][0])\n        rhs=\"_\".join(row[1][1])\n        conf=row[1][3]; #print(conf)\n        if(lhs not in G.nodes): \n            G.add_node(lhs)\n        if(rhs not in G.nodes): \n            G.add_node(rhs)\n\n        edge=(lhs,rhs)\n        if edge not in G.edges:\n            G.add_edge(lhs, rhs, weight=conf)\n\n    # print(G.nodes)\n    # print(G.edges)\n    return G\n\n\n\n\nCode\ndef plot_network(G):\n    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n    pos=nx.random_layout(G)\n\n    #GENERATE PLOT\n    fig, ax = plt.subplots()\n    fig.set_size_inches(45, 45)\n\n    #assign colors based on attributes\n    weights_e   = [G[u][v]['weight'] for u,v in G.edges()]\n\n    #SAMPLE CMAP FOR COLORS \n    cmap=plt.cm.get_cmap('Blues')\n    colors_e    = [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n\n    #PLOT\n    nx.draw(\n    G,\n    node_color='crimson',\n    edgecolors=\"black\",\n    edge_color=colors_e,\n    node_size=7000,\n    linewidths=2,\n    font_size=12,\n    font_color=\"black\",\n    width=weights_e,\n    with_labels=True,\n    pos=pos,\n    ax=ax\n    )\n    ax.set_title(\"NetworkX Graph for Association Rules\", fontsize=50)\n    \n    plt.show()\n\n# raise\n\n\nFINAL RESULTS\n\n\nCode\nresult_df = reformat_results(association_results)\nprint(\"Results\\n\",len(association_results))\n\n\nResults\n 46\n\n\nVISUALISATION\n\n\nCode\nG = convert_to_network(result_df)\nplot_network(G)\n\n\n\n\n\nCONCLUSION\nFrom the network graph, we can see how the locations are associated with different type of crimes for different year. When considering the two main crime category, the graphs shows us that there is association with two different crime types also, which also tells us that there is association between withon each category of crime."
  },
  {
    "objectID": "crd.html",
    "href": "crd.html",
    "title": "CLUSTERING FOR RECORD DATA",
    "section": "",
    "text": "INTRODUCTION\nThe objective is to identify different groupings or “clusters” within a dataset for the violent crime category. Using a machine language algorithm, the tool builds groups in which items inside a comparable group have, on average, similar features.\nABOUT THE DATA\nThe data here emerges from an FBI source that keeps track of crime rates in each state from 1960 to 2019. The data includes two types of crime: property crime rate and violent crime rate. However, we will focus mostly on violent crime rate because that is what the decision tree analysis was based on. There are four different types of violent crime: assault, rape, robbery, and murder.\nDATA CLEANING AND VISUALIZATION\nDuring the data cleaning procedure, raw data was cleansed, and the cleaned data is then input for the clustering procedure. As violent crime is the primary focus of the clustering, we extract the violent crime rate data from the dataset into a separate dataset and then visualize the data to determine the relationship between the variables.\nIMPORT LIBRARIES\n\n\nCode\n# Packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\", palette='Set2')\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial.distance import cdist\nimport warnings\nwarnings.filterwarnings('ignore')\n# Importing relevent libraries for clustering. \n# We will use KMeans, AgglomerativeClustering, MeanShift, Birch, and DBSCAN\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nfrom itertools import cycle\nfrom sklearn.cluster import Birch\n\n\nIMPORT DATA\n\n\nCode\n# Read the data\ndf = pd.read_csv(\"../../data/modified-data/cleaned_state_crime_record_data.csv\")\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      State\n      Year\n      Population\n      Property_Crime_Rate\n      Property_Burglary_Rate\n      Property_Larceny_Rate\n      Property_Motor_Rate\n      Violent_Crime_Rate\n      Violent_Assault_Rate\n      Violent_Murder_Rate\n      Violent_Rape_Rate\n      Violent_Robbery_Rate\n    \n  \n  \n    \n      0\n      Alabama\n      1960\n      3266740\n      1035.4\n      355.9\n      592.1\n      87.3\n      186.6\n      138.1\n      12.4\n      8.6\n      27.5\n    \n    \n      1\n      Alabama\n      1961\n      3302000\n      985.5\n      339.3\n      569.4\n      76.8\n      168.5\n      128.9\n      12.9\n      7.6\n      19.1\n    \n    \n      2\n      Alabama\n      1962\n      3358000\n      1067.0\n      349.1\n      634.5\n      83.4\n      157.3\n      119.0\n      9.4\n      6.5\n      22.5\n    \n    \n      3\n      Alabama\n      1963\n      3347000\n      1150.9\n      376.9\n      683.4\n      90.6\n      182.7\n      142.1\n      10.2\n      5.7\n      24.7\n    \n    \n      4\n      Alabama\n      1964\n      3407000\n      1358.7\n      466.6\n      784.1\n      108.0\n      213.1\n      163.0\n      9.3\n      11.7\n      29.1\n    \n  \n\n\n\n\n\n\nCode\n#create new column\ndf['Violent_label'] = df[['Violent_Assault_Rate', 'Violent_Murder_Rate','Violent_Rape_Rate','Violent_Robbery_Rate']].idxmax(axis=1)\n#create table for only property crime datasets\nviolent_df = df[['Violent_Assault_Rate', 'Violent_Murder_Rate','Violent_Rape_Rate','Violent_Robbery_Rate','Violent_label']].copy()\n\n\nDATA VISUALISATION\n\n\nCode\n#visualize the data\nfigsize=(30,10)\npairplot = sns.pairplot(violent_df)\npairplot.fig.suptitle('Pairplot for Violent Crime Rate from 1960-2019', fontsize = 15)\n\n\nText(0.5, 0.98, 'Pairplot for Violent Crime Rate from 1960-2019')\n\n\n\n\n\nAccording to the pairplot, there is a positive correlation between all variables.\nCLUSTERING\nClustering is the process of splitting a population or set of data points into many groups in which the data points within each group are more similar to each other than to the data points within any of the other groups. This ensures that each group has data points that are more comparable to one another than to any other group’s data points. It is, at its most basic level, a collection of items grouped according to the degrees to which they are similar to or distinct from the other items in the collection. It achieves this by scanning the unlabeled dataset for recurrent patterns, such as shape, size, color, and behavior, and then splitting the data depending on the presence or absence of these recurring patterns. Due to the nature of unsupervised learning, the algorithm receives no supervision and operates on unlabeled datasets. In addition, the datasets lack labels.\nCLUSTERING ALGORITHMS:\nClustering techniques can be utilized to categorize data points into groups based on their shared commonalities with other data points. There is no established set of clustering success criteria. Clustering is a methodology for classifying unlabeled data. It depends largely on the particular user and the circumstances.\nCommon cluster models:\n\nConnectivity models, which construct models based on distance connectedness.\nCentroid models, which represent each cluster with a single mean vector\nDistribution models, which model clusters using statistical distributions\nDensity models, which defines clustering as a densely connected region in data space.\n\nKMEANS\nK-means clustering is one of the simplest and most often used algorithms for unsupervised learning. A cluster is a collection of data items that are grouped together because they have certain characteristics. You will set a goal number, k, which corresponds to the required number of centers in the dataset. A centroid is a physical or fictitious location that marks the cluster’s center. By reducing the sum of squares for each cluster, each data point is assigned to one of the clusters. In other words, the K-means algorithm identifies k centers, then places each data point in the cluster that is closest to it while minimizing the size of the centers. The term “means” in K-means refers to determining the center or arithmetic mean of the data.\nAdvantages of k-means:\n\nRelatively straightforward to implement.\nScales to big data collections.\nGuarantees convergence.\nPossibility to warm-up the positions of centroids.\nAdapts readily to new examples.\nGeneralizes to clusters of various sizes and forms, including elliptical clusters.\n\nDisadvantages of k-means:\n\nChoosing k manually.\nData of varied sizes and densities are clustered.\nClustering outliers.\nScaling with dimension count.\n\nCLUSTERING WITH RANDOM HYPER - PARAMETER: KMEANS ALGORITHM\nA random K value is taken. Here, k = 3 is employed as the initial label prediction step. Once the labels have been predicted, kmeans model clustering with the same k value, k = 3, is performed to examine the clusters.\n\n\nCode\n# Split the dataset in X and y. \n# Since this is unsupervised learning, we will not use the y labels. \n# Normalizing the X data by using the StandardScaler function.\n\nX = violent_df.drop('Violent_label', axis=1)\ny = violent_df['Violent_label']\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\nKMEANS CLUSTERING FOR K = 3\n\n\nCode\n# For k means clustering we will use the elbow method to find the optimal number of clusters. \n# we will use the inertia_ attribute to find the sum of squared distances of samples to their closest cluster center. \n# we will use the range of 1 to 10 clusters. plot the inertia_ values for each number of clusters. \n# make sure to save it in a dataframe and plot it using matplotlib.\n\ninertia = pd.DataFrame(columns=['clusters', 'inertia', 'distortion'])\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, random_state=0)\n    kmeans.fit(X)\n    inertia = inertia.append({'clusters': i, 'inertia': kmeans.inertia_, 'distortion': sum(np.min(cdist(X, kmeans.cluster_centers_, 'euclidean'), axis=1))/X.shape[0]}, ignore_index=True)\ninertia\n\n\n\n\n\n\n  \n    \n      \n      clusters\n      inertia\n      distortion\n    \n  \n  \n    \n      0\n      1.0\n      12460.0\n      1.539448\n    \n    \n      1\n      2.0\n      8190.368723\n      1.228229\n    \n    \n      2\n      3.0\n      5301.55608\n      1.123453\n    \n    \n      3\n      4.0\n      4327.735197\n      0.986535\n    \n    \n      4\n      5.0\n      3812.255194\n      0.911249\n    \n    \n      5\n      6.0\n      3365.198723\n      0.890649\n    \n    \n      6\n      7.0\n      2946.399983\n      0.843668\n    \n    \n      7\n      8.0\n      2640.763725\n      0.80562\n    \n    \n      8\n      9.0\n      2450.515735\n      0.767369\n    \n    \n      9\n      10.0\n      2264.273317\n      0.728758\n    \n  \n\n\n\n\n\n\nCode\n# plot distortion and inertia for kmeans, \n# you can either plot them seperately or use fig, ax = plt.subplots(1, 2) to plot them in the same figure. \n# Suggest the optimal number of clusters based on the plot.\n\ninertia.plot.line(x=\"clusters\", subplots=True)\n\n\narray([<AxesSubplot:xlabel='clusters'>, <AxesSubplot:xlabel='clusters'>],\n      dtype=object)\n\n\n\n\n\n\n\nCode\nkmeans = KMeans(n_clusters=3, random_state=123)\nkmeans.fit(X)\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\nplt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, alpha=0.5)\nplt.title('K-Means Clustering for K=3')\nplt.show()\n\n\n\n\n\nHYPER-PARAMTER TUNING:\nTuning hyper - parameters for unsupervised learning problems is often difficult due to a lack of validation ground truth. However, the bulk of clustering algorithms rely heavily on the selection of proper hyper - parameters. Numerous hyper - parameters must be given in advance for each method that conducts Clustering on a dataset. These hyper-parameters, however, must be adjusted with our dataset in mind. Using random hyper - parameters that do not match our dataset may result in improper clustering of datapoints. As a result, hyper-parameters are used to optimize their performance.\nTo perform hyper-parameter tuning, we must have functions employ a measure to identify the best Hyper-parameter (s). This statistic varies depending on the algorithm and the procedure within the algorithm. There are numerous methods for determining optimal value, including the Elbow technique, the Silhouette approach, the Grubbs method, and so on.\nHYPER-PARAMTER TUNING FOR KMEANS:\nFor K-Means Algorithm, hyper-parameter is n_cluster. There are two metods which are being used to find the optimal value:\n\nELBOW METHOD\n\nBy fitting the model with a range of values for, the “elbow” method can help choose the best number of clusters. If the line chart looks like an arm, the “elbow,” or point where the curve bends, is a good sign that the model fits best at that point. “Elbow” will be marked in the visualizer with a dashed line.\nHere, the k value is considered to range from 2 to 7. When the model is fit, we can see a line on the graph that marks the “elbow,” which in this case is the best number.\n\n\nCode\n#Look at best values for k \nSS_dist = []\n\nvalues_for_k=range(2,7)\n\nfor k_val in values_for_k:\n    k_means = KMeans(n_clusters=k_val)\n    model = k_means.fit(X)\n    SS_dist.append(k_means.inertia_)\n    \n\nplt.plot(values_for_k, SS_dist, 'bx-', color='lightblue')\nplt.xlabel('value')\nplt.ylabel('Sum of squared distances')\nplt.title('Elbow method for optimal k Choice')\nplt.show()\n\n\n\n\n\n\nSILHOUETTE METHOD\n\nThe silhouette method can also be used to find the best number of clusters or other hyper-parameters and to check that the data in each cluster is consistent. This method figures out silhouette coefficients for each sample point and takes the average of all of them to get the silhouette score. It picks the set of hyper-parameters with the highest silhouette score. The silhouette value is a way to compare how similar an object is to other objects in its own cluster (separation).\nHere, the k value is considered to range from 2 to 7. When the model is fit, we can see a line on the graph that marks the “silhouette,” which in this case is the best number.\n\n\nCode\n# Look at Silhouette\nSih=[]\nCal=[]\nk_range=range(2,20)\n\nfor k in k_range:\n    k_means_n = KMeans(n_clusters=k)\n    model = k_means_n.fit(X)\n    Pred = k_means_n.predict(X)\n    labels_n = k_means_n.labels_\n    R1=metrics.silhouette_score(X, labels_n, metric = 'euclidean')\n    R2=metrics.calinski_harabasz_score(X, labels_n)\n    Sih.append(R1)\n    Cal.append(R2)\n\n\nfig1, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,10))\nax1.plot(k_range,Sih, color=\"#8B8386\")\nax1.set_title(\"Silhouette\")\nax1.set_xlabel(\"\")\n\nax2.plot(k_range,Cal, color='#F08080')\nax2.set_title(\"Calinski Harabasz Score\")\nax2.set_xlabel(\"k values\")\n\n\nText(0.5, 0, 'k values')\n\n\n\n\n\nFrom the above methods, the optimal value is taken as 2 i.e. k = 2. Considering k =2, the data is fit into the k-means algorithm and proceeding with the scatter plot. Principal component analysis (PCA) is a way to reduce the number of dimensions in these kinds of datasets, making them easier to understand while losing as little information as possible. It does this by making new variables that are not related to each other and that gradually optimize variance.\nK-MEAN CLUSTERING FOR K = 2\n\n\nCode\nkmeans = KMeans(n_clusters=2, random_state=123)\nkmeans.fit(X)\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\nplt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, alpha=0.5)\nplt.title('K-Means Clustering for K=2')\nplt.show()\n\n\n\n\n\nDBSCAN: Density-Based Spatial Clustering of Applications with Noise\nDBSCAN is a density-based clustering technique that operates on the premise that clusters are clumps of data physically separated by less dense areas. It constructs a cluster from ‘densely clustered’ data points. It can locate groups in big spatial datasets by evaluating the regional density of the data points. The resistance to outliers is a noteworthy feature of DBSCAN clustering. It does not require us to know the number of clusters in advance, unlike K-Means, which requires us to supply the number of centroids.\nThe DBSCAN algorithm is described in detail below:\n\nDBSCAN generates its initial data point at random (non-visited points).\nThis point’s neighborhood is extracted using an epsilon distance.\nIf there are enough data points in this region, the clustering mechanism initiates, and the current data point becomes the first point in the newest cluster; otherwise, it is classified as noise and is subsequently visited.\nEvery other point within epsilon distance of the first point in the new cluster likewise joins it as a member of the same cluster. The steps taken to ensure that all previously added data points are also part of the same cluster are repeated for all newly added data points.\nThe preceding two procedures are continued until all cluster nodes are identified. All of the points in the cluster’s immediate neighborhood have been explored and categorized. When we’ve finished with the current cluster, we’ll move on to the next one by retrieving and processing a previously unvisited point. This process is carried out until all of the data points have been checked off as visited.\n\nAdvantages of DSCAN:\n\nDoesn’t need the number of clusters to be set up front.\nAble to tell when data is just noise while clustering.\nThe DBSCAN algorithm can find clusters that are any size and any shape.\n\nDisadvantages of DBSCAN:\n\nWhen clusters have different densities, the DBSCAN algorithm doesn’t work.\nFails if the dataset is a neck type.\n\nIn DBSCAN, the hyperparameters are Min points and epsilon.\n\nMin points: Min points ≥ dimensionality +1\n\nIf the set of data is more noisy, we use Min. Points are bigger because it’s easy to get rid of noisy ones.\n\nRadius (Epsilon): Elbow method\n\nWe figure out the distance between each data point and then sort distances from farthest to closest, and then draw a graph between distance and point index. From the graph,we choose the best distance (epsilon) where the graph shows a sharp rise.\n\n\nCode\n# we use nearestneighbors for calculating distance between points\nfrom sklearn.neighbors import NearestNeighbors\n\n# calculating distances\nneigh=NearestNeighbors(n_neighbors=2)\ndistance=neigh.fit(X)\n# indices and distance values\ndistances,indices=distance.kneighbors(X)\n# Now sorting the distance increasing order\nsorting_distances=np.sort(distances,axis=0)\n# sorted distances\nsorted_distances=sorting_distances[:,1]\n# plot between distance vs epsilon\nplt.plot(sorted_distances, color='lightblue')\nplt.xlabel(\"Distance\")\nplt.ylabel(\"Epsilon\")\nplt.show()\n\n\n\n\n\n\n\nCode\n# DBSCAN Clustering\n# perform DBSCAN clustering. use the eps and min_samples parameters to find the optimal number of clusters. \ndbscan_df = pd.DataFrame(columns=['eps', 'min_samples', 'clusters', 'silhouette_score'])\n\nfor i in np.arange(0.1, 2.1, 0.1):\n    \n    for j in range(1, 11):\n        \n        dbscan = DBSCAN(eps=i, min_samples=j)\n        dbscan.fit(X)\n        \n        if (len(set(dbscan.labels_) - set([-1])) > 1) & (len(set(dbscan.labels_) - set([-1])) < 11):\n            dbscan_df = dbscan_df.append({'eps': i, 'min_samples': j, 'clusters': len(set(dbscan.labels_) - set([-1])), 'silhouette_score': silhouette_score(X, dbscan.labels_)}, ignore_index=True)\n        \n        else:\n            dbscan_df = dbscan_df.append({'eps': i, 'min_samples': j, 'clusters': 0, 'silhouette_score': 0}, ignore_index=True)\n        \n\n\n\n\nCode\n# plot the number of clusters vs the silhouette score. Suggest the optimal number of clusters based on the plot.\ndbscan_df.plot.line(x='clusters', y='silhouette_score')\nplt.title(\"Silhouette Score vs Number of Clusters\")\n\n\nText(0.5, 1.0, 'Silhouette Score vs Number of Clusters')\n\n\n\n\n\nDBSCAMN LABELS\n\n\nCode\noptimal_cluster_size = dbscan_df['clusters'][dbscan_df['silhouette_score'] == max(dbscan_df['silhouette_score'])]\ndbscan = DBSCAN(eps=1.7, min_samples=10)\ndbscan.fit(X)\ny_pred = dbscan.fit_predict(X)\nlabels_DB = dbscan.labels_\nprint(labels_DB)\n\n\n[0 0 0 ... 0 0 0]\n\n\nNUMBER OF POINTS ON EACH CLUSTER\n\n\nCode\nimport collections\ncollections.Counter(labels_DB)\n\n\nCounter({0: 3071, -1: 34, 1: 10})\n\n\nDBSCAN CLUSTER VISUALISATION\n\n\nCode\n#plot clusters using PCA\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca.fit(X)\nX_pca = pca.transform(X)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_DB, s=50, cmap='viridis')\nplt.xlabel('clusters')\nplt.title('DBSCAN Clusters')\nplt.ylabel('Measures')\nplt.show()\n\n\n\n\n\nSILHOUETTE COEFFICIENT\n\n\nCode\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels_DB))\n\n\nSilhouette Coefficient: 0.707\n\n\nHIERARCHIAL CLUSTERING\nHierarchical clustering methods join and divide existing clusters repeatedly to create multilayer clusters. This structure, in the form of a tree, displays the relative hierarchy of the groups (or dendrogram). The trunk of the tree is the single cluster that contains all of the others, while the leaves are the many other clusters that each contain exactly one sample.\nAgglomerative hierarchical clustering (AHC):\nWith the AgglomerativeClustering object, you can do hierarchical clustering from the bottom up. This means that each observation is put into its own cluster before being combined with others. AgglomerativeClustering can be used with a large number of samples when paired with a connectivity matrix. However, it has a high computational cost when there are no restrictions on the connections between samples because it looks at all possible mergers at each step.\nAdvantages of AHC:\n\nAHC is easy to set up, and it can also arrange objects in a way that is helpful for the display.\nWe don’t have to know ahead of time how many clusters there will be. By cutting the dendrogram at a certain level, it’s easy to figure out how many clusters there are.\nIn the AHC method, smaller groups of data will be put together, which may show similarities.\n\nDisadvantages of AHC:\n\nIf you group the objects wrong in any of the first steps, you can’t go back and fix it.\nHierarchical clustering algorithms don’t give a unique way to divide the dataset, but they do give a hierarchy that can be used to choose which clusters to use.\nThey don’t do a good job with outliers. When outliers are found, they can lead to the formation of a new cluster or the merging of two or more clusters.\n\nThere are two key concepts in hierarchical clustering:\n\nThe bottom-up implementation of this algorithm is described above. Another option is to work from the top down, initially placing all data points in the same cluster before recursively splitting them into their own groups.\nClusters are merged based on how near they are to one another.\n\nHere,The Euclidean distance between the points is used to do aglomerative clustering for 4 clusters. Followed by prediction of labels and plotting dendrogram for the data.\n\n\nCode\n# Agglomerative Clustering - Hierarchical Clustering\n\n# Perform Agglomerative Clustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\n\nmodel = AgglomerativeClustering().fit(X)\nlabels = model.labels_\n\n\n\n\nCode\n# create linkage for agglomerative clustering, and the dendrogram for the linkage. The optimal number of clusters based on the dendrogram.\n\nZ = linkage(X, method='ward')\ndend = dendrogram(Z)\nfigsize =(12, 12)\nplt.axhline(y=62, color='r', linestyle='--', label='optimal number of clusters')\nplt.title('Dendrogram')\nplt.xlabel('Clusters')\nplt.ylabel('Euclidean Distance')\nplt.legend()\nplt.show()\n\n\n\n\n\nRESULTS\n\nFrom the Kmeans Algorithm, the dataset is 2 groups using the unsupervised k-means algorithm.\nFrom the DBSCAN Algorithm, the number of clusters are 3, the Silhouette Coefficient is 0.7, which means that the data point i is very compact within the cluster to which it belongs and far away from the other clusters as the value is near 1.\nFrom Hierarchical Algorithm, the Euclidean distance method was used to make a dendrogram with k = 4. As can be seen, the clusters are very close to each other.The red dotted line indicates that the number of cluster is 3.\n\nCONCLUSION\nIn the he violent crime category dataset, the label column based on the various crime category such as murder, rape, robbery and assault. The aim is to look for connections between the crimes. Clustering the dataset provided insights into that. The value of k = 2 was set for defining the number of clusters which was calculated using the Elbow method and Silhouette method. From the above plots, we can conclude that the clusters are overlapping and can be improved."
  },
  {
    "objectID": "ctd.html",
    "href": "ctd.html",
    "title": "CLUSTERING FOR TEXT DATA",
    "section": "",
    "text": "INTRODUCTION\nA data collection effort for the hashtags “CRIME” is now being carried out on Twitter. The goal of this is to have a deeper comprehension of the many viewpoints that people hold about criminal activity.\nDATA CLEANING AND VISUALIZATION\nCleaning up the text data from Twitter is done with Python. In order to carry out an investigation into #crime, all of the columns in the dataset were taken out, leaving only the text column. This allowed for the conduct of an investigation. During this stage of the cleaning process, both the Tokenization module from the NLTK library and the Countvectorizer module from the scikit-learn library are utilized.\nThe following bar graph and word cloud display the results of data visualization performed on the #CRIME dataset. These display the most frequently used tweet words along with the hashtag. The wordcloud reveals that the most frequently used terms associated with the hashtag are “inflation”, “breakthrough”, “mass”, and “dating” which provides some insight into the context in which people are using the hashtag.\nIMPORT LIBRARIES\n\n\nCode\n\n#Loading the required libaries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nimport warnings\nfrom sys import exit\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nfrom sklearn import preprocessing\nimport pylab as pl\nfrom sklearn import decomposition\nimport seaborn as sns\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as hc\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import DBSCAN\n\n\nIMPORT DATA\n\n\nCode\ndf = pd.read_csv(\"../../data/modified-data/cleaned_crime_text_data.csv\")\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      text\n      clean text\n      Tweet_tokenized\n      Tweet_without_stop\n      Tweet_stemmed\n      Tweet_lemmatized\n    \n  \n  \n    \n      0\n      @WhatRickySaid @CalaVento @memestheband @HongF...\n      WhatRickySaid CalaVento memestheband HongFaux ...\n      ['whatrickysaid', 'calavento', 'memestheband',...\n      ['whatrickysaid', 'calavento', 'memestheband',...\n      ['whatrickysaid', 'calavento', 'memestheband',...\n      ['whatrickysaid', 'calavento', 'memestheband',...\n    \n    \n      1\n      Breakthrough in mass murder case\\nhttps://t.co...\n      Breakthrough in mass murder case\\nhttpstcoBhdF...\n      ['breakthrough', 'in', 'mass', 'murder', 'case...\n      ['breakthrough', 'mass', 'murder', 'case', 'ht...\n      ['breakthrough', 'mass', 'murder', 'case', 'ht...\n      ['breakthrough', 'mass', 'murder', 'case', 'ht...\n    \n    \n      2\n      Driven by Kerena Swan @KerenaSwan @rararesourc...\n      Driven by Kerena Swan KerenaSwan rararesources...\n      ['driven', 'by', 'kerena', 'swan', 'kerenaswan...\n      ['driven', 'kerena', 'swan', 'kerenaswan', 'ra...\n      ['driven', 'kerena', 'swan', 'kerenaswan', 'ra...\n      ['driven', 'kerena', 'swan', 'kerenaswan', 'ra...\n    \n    \n      3\n      Suspicious phone call? Something doesn't feel ...\n      Suspicious phone call Something doesnt feel ri...\n      ['suspicious', 'phone', 'call', 'something', '...\n      ['suspicious', 'phone', 'call', 'something', '...\n      ['suspici', 'phone', 'call', 'someth', 'doesnt...\n      ['suspicious', 'phone', 'call', 'something', '...\n    \n    \n      4\n      @RDTVF Donate @RDTVF #indie #internet #radio s...\n      RDTVF Donate RDTVF indie internet radio suppor...\n      ['rdtvf', 'donate', 'rdtvf', 'indie', 'interne...\n      ['rdtvf', 'donate', 'rdtvf', 'indie', 'interne...\n      ['rdtvf', 'donat', 'rdtvf', 'indi', 'internet'...\n      ['rdtvf', 'donate', 'rdtvf', 'indie', 'interne...\n    \n  \n\n\n\n\nVISUALIZATION\n\n\nCode\ntwitter_data = df[['Tweet_lemmatized']]\n#remove retweets\ntwitter_data = twitter_data[~twitter_data['Tweet_lemmatized'].str.contains('rt')]\n#BARGRAPH OF TOP USED WORDS IN TWEETS\ndef get_top_n_words(corpus, n=None):\n    vec=CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_words(twitter_data['Tweet_lemmatized'], 20)\ntwitter_data_count = pd.DataFrame(common_words, columns = ['Review', 'count'])\n\ntwitter_data_count.groupby('Review').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar',\n    figsize=(10, 6),\n    xlabel = \"Top Words\",\n    ylabel = \"Count\",\n    title = \"Bar Chart of Top Words Frequency\", color=\"#528B8B\"\n)\n\n\n<AxesSubplot:title={'center':'Bar Chart of Top Words Frequency'}, xlabel='Top Words', ylabel='Count'>\n\n\n\n\n\nWORDCLOUD\n\n\nCode\n#WORDCLOUD OF THE #CRIME DATASETS\nstopwords = set(STOPWORDS)\nletters_only = re.sub(\"[^a-zA-Z]+\", \" \", str(twitter_data['Tweet_lemmatized']))\n\nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10,  colormap=\"bone\").generate(letters_only)\nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()\n\n\n\n\n\nCOUNT OF LEMMATIZED TWEETS\n\n\nCode\n#count tweet['lemmatized'] rows\ntwitter_data['Tweet_lemmatized'].count()\n\n\n3297\n\n\nCOUNT VECTORIZER\nThe Python scikit-learn module CountVectorizer is used to turn a supplied text into a vector based on the frequency of each word that appears in the full text. This is useful when we have numerous such texts and want to transform each word to a vector for using in text analysis. CountVectorizer generates a matrix where each distinct word corresponds to a column so each text sample in the document corresponds to a row. Each cell’s value is just the number of words in its respective text sample. After applying the count vectorizer, the data are restored and used for further clustering analysis.\n\n\nCode\n#COUNTVECTORIZER\nCV=CountVectorizer(input='content',\n                      stop_words='english',\n                      #decode_error='ignore'\n                      )\nThe_DTM_CV=CV.fit_transform(twitter_data['Tweet_lemmatized'])\nTheColumnNames_CV=CV.get_feature_names()\n\n#regex model\npattern = r'[0-9]'\n\n# Match all digits in the string and replace them with an empty string\nNew_TheColumnNames = re.sub(pattern, '', str(TheColumnNames_CV))\n\n#The second step is to use pandas to create data frames\nThe_DF = pd.DataFrame(The_DTM_CV.toarray(),columns=TheColumnNames_CV)\nThe_DF = The_DF.filter(regex='^\\D')\n\nThe_DF.to_csv(\"DTM_CV.csv\")\n#DATA AFTER COUNTVECTORIZER\nfilename= pd.read_csv(\"DTM_CV.csv\")\n\n#normalize the data\nfile_norm =(filename - filename.mean()) / filename.std()\nfilename.head()\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      aadhi\n      aaditya\n      aaftab\n      aaftabpoonawalla\n      aap\n      aapl\n      aaronovitch\n      aarop\n      aatma\n      ...\n      𝗛𝗼𝘂𝘀𝗲𝘄𝗶𝗳𝗲\n      𝗞𝗶𝗹𝗹𝗶𝗻𝗴\n      𝗟𝗢𝗡𝗚\n      𝗢𝗡\n      𝗣𝗲𝘀𝘁𝗹𝗲\n      𝗣𝗼𝗹𝗶𝗰𝗲\n      𝗦𝗛𝗔𝗗𝗢𝗪\n      𝗦𝗧𝗔𝗚𝗘\n      𝗧𝗛𝗘\n      𝗪𝗶𝘁𝗵\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 12018 columns\n\n\n\nCLUSTERING\nClustering is the process of dividing a population or set of data points into a number of groups in such a way that the data points in each group are more similar to each other than they are to the data points in any of the other groups. This ensures that each group contains data points that are more comparable to each other than they are to the data points in any of the other groups. At its most fundamental level, it is a collection of items that are arranged in accordance with the degrees to which they are similar to or distinct from the other items in the collection. It accomplishes this by searching the unlabeled dataset for recurring patterns, such as shape, size, color, and behavior, among other things, and then dividing the data based on whether or not those recurring patterns are present or absent. Because it is an unsupervised learning approach, the algorithm does not receive any supervision, and it works with unlabeled datasets. Also, the datasets are not labeled.\nCLUSTERING ALGORITHMS:\nClustering algorithms can be used to organize data points into groups according to the similarities they share with other data points. There is not a set of criteria that define successful clustering. Clustering is a method for categorizing data that is not pre-labeled. It is mostly dependent on the individual user as well as the circumstance.\nCommon cluster models:\n\nConnectivity models, which construct models based on distance connectedness.\nCentroid models, which represent each cluster with a single mean vector\nDistribution models, which model clusters using statistical distributions\nDensity models, which defines clustering as a densely connected region in data space.\n\nKMEANS\nK-means clustering is one of the easiest and most popular algorithms for learning without being watched. A cluster is a group of data points that are put together because they have some things in common. You will set a target number, k, which is the number of centers you need in the dataset. A centroid is the place, real or made up, that represents the cluster’s center. Each data point is put into one of the clusters by lowering the sum of squares for each cluster. In other words, the K-means algorithm finds k number of centers, then puts each data point in the cluster that is closest to it while keeping the centers as small as possible. The word “means” in K-means refers to taking the average of the data, or finding the center.\nAdvantages of k-means:\n\nRelatively straightforward to implement.\nScales to big data collections.\nGuarantees convergence.\nPossibility to warm-up the positions of centroids.\nAdapts readily to new examples.\nGeneralizes to clusters of various sizes and forms, including elliptical clusters.\n\nDisadvantages of k-means:\n\nChoosing k manually.\nData of varied sizes and densities are clustered.\nClustering outliers.\nScaling with dimension count.\n\nCLUSTERING WITH RANDOM HYPER - PARAMETER: KMEANS ALGORITHM\nA random K value is taken. Here, k = 3 is employed as the initial label prediction step. Once the labels have been predicted, kmeans model clustering with the same k value, k = 3, is performed to examine the clusters.\nPREDICTION OF LABELS\n\n\nCode\n# KMEANS\n# Use k-means clustering on the data.\nk = 3\n## Sklearn required you to instantiate first\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(filename)   ## run kmeans\n\nlabels = kmeans.labels_\nprint(labels)\n\ncentroids = kmeans.cluster_centers_\nprint(centroids)\n\nprediction = kmeans.predict(filename)\nprint(prediction)\n\n\n[1 1 1 ... 2 2 2]\n[[ 1.64700000e+03 -2.71050543e-19 -2.71050543e-19 ... -2.71050543e-19\n  -5.42101086e-19 -2.71050543e-19]\n [ 5.48500000e+02 -1.13841228e-18 -1.13841228e-18 ... -1.13841228e-18\n  -2.27682456e-18  9.10746812e-04]\n [ 2.74650000e+03  9.09090909e-04  9.09090909e-04 ...  9.09090909e-04\n   1.81818182e-03  1.02999206e-18]]\n[1 1 1 ... 2 2 2]\n\n\nKMEANS CLUSTERING FOR K = 3\n\n\nCode\n#Kmeans for K = 3\nkm = KMeans(n_clusters=3)\nkm.fit(The_DTM_CV)\npca = PCA(n_components=2, random_state=2)\nreduced_features = pca.fit_transform(The_DTM_CV.toarray())\n\n# reduce the cluster centers to 2D\\\nreduced_cluster_centers = pca.transform(km.cluster_centers_)\nfrom matplotlib.pyplot import figure\n\nfigure(figsize=(10, 12), dpi=80)\nplt.scatter(reduced_features[:,0], reduced_features[:,1], c=km.predict(The_DTM_CV))\nplt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='r')\n\n\n<matplotlib.collections.PathCollection at 0x7f8b34878430>\n\n\n\n\n\nHYPER-PARAMTER TUNING:\nTuning hyper - parameters for unsupervised learning issues is typically difficult owing to the lack of validation ground truth. However, the effectiveness of the majority of clustering algorithms relies greatly on the selection of appropriate hyper - parameters. For each method that performs Clustering on a dataset, numerous hyper - parameters must be specified in advance. However, these hyper-parameters must be modified with our dataset in mind. Using random hyper - parameters that do not match our dataset could result on incorrect classification of datapoints into clusters. So, in  order to optimize their performance hyper-parameters are being used.\nTo do hyper-parameter tuning, we have to make functions  use a metric to try to find the best Hyper-parameter(s). This metric is different for each algorithm and each method within an algorithm. There are many ways to figure out optimal value such as the Elbow method, the Silhouette method, the Grubbs method, and so on.\nHYPER-PARAMTER TUNING FOR KMEANS:\nFor K-Means Algorithm, hyper-parameter is n_cluster. There are two metods which are being used to find the optimal value:\n\nELBOW METHOD\n\nBy fitting the model with a range of values for, the “elbow” method can help choose the best number of clusters. If the line chart looks like an arm, the “elbow,” or point where the curve bends, is a good sign that the model fits best at that point. “Elbow” will be marked in the visualizer with a dashed line.\nHere, the k value is considered to range from 2 to 7. When the model is fit, we can see a line on the graph that marks the “elbow,” which in this case is the best number.\n\n\nCode\n#Look at best values for k \nSS_dist = []\n\nvalues_for_k=range(2,7)\n\nfor k_val in values_for_k:\n    k_means = KMeans(n_clusters=k_val)\n    model = k_means.fit(filename)\n    SS_dist.append(k_means.inertia_)\n    \n\nplt.plot(values_for_k, SS_dist, 'bx-', color='lightblue')\nplt.xlabel('value')\nplt.ylabel('Sum of squared distances')\nplt.title('Elbow method for optimal k Choice')\nplt.show()\n\n\n\n\n\n\nSILHOUETTE METHOD\n\nThe silhouette method can also be used to find the best number of clusters or other hyper-parameters and to check that the data in each cluster is consistent. This method figures out silhouette coefficients for each sample point and takes the average of all of them to get the silhouette score. It picks the set of hyper-parameters with the highest silhouette score. The silhouette value is a way to compare how similar an object is to other objects in its own cluster (separation).\nHere, the k value is considered to range from 2 to 7. When the model is fit, we can see a line on the graph that marks the “silhouette,” which in this case is the best number.\n\n\nCode\n# Look at Silhouette\nSih=[]\nCal=[]\nk_range=range(2,20)\n\nfor k in k_range:\n    k_means_n = KMeans(n_clusters=k)\n    model = k_means_n.fit(filename)\n    Pred = k_means_n.predict(filename)\n    labels_n = k_means_n.labels_\n    R1=metrics.silhouette_score(filename, labels_n, metric = 'euclidean')\n    R2=metrics.calinski_harabasz_score(filename, labels_n)\n    Sih.append(R1)\n    Cal.append(R2)\n\n\nfig1, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,10))\nax1.plot(k_range,Sih, color=\"#8B8386\")\nax1.set_title(\"Silhouette\")\nax1.set_xlabel(\"\")\n\nax2.plot(k_range,Cal, color='#F08080')\nax2.set_title(\"Calinski_Harabasz_Score\")\nax2.set_xlabel(\"k values\")\n\n\nText(0.5, 0, 'k values')\n\n\n\n\n\nFrom the above methods, the optimal value is taken as 2 i.e. k = 2. Considering k =2, the data is fit into the k-means algorithm and proceeding with the scatter plot. Principal component analysis (PCA) is a way to reduce the number of dimensions in these kinds of datasets, making them easier to understand while losing as little information as possible. It does this by making new variables that are not related to each other and that gradually optimize variance.\n\n\nCode\n#Kmeans for K = 2\nkm = KMeans(n_clusters=2)\nkm.fit(The_DTM_CV)\npca = PCA(n_components=2, random_state=2)\nreduced_features = pca.fit_transform(The_DTM_CV.toarray())\n\n# reduce the cluster centers to 2D\\\nreduced_cluster_centers = pca.transform(km.cluster_centers_)\nfrom matplotlib.pyplot import figure\n\nfigure(figsize=(10, 12), dpi=80)\nplt.scatter(reduced_features[:,0], reduced_features[:,1], c=km.predict(The_DTM_CV), cmap='rainbow')\nplt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='r', cmap='rainbow')\n\n\n<matplotlib.collections.PathCollection at 0x7f8b12b1da00>\n\n\n\n\n\nDBSCAN: Density-Based Spatial Clustering of Applications with Noise\nDBSCAN is an algorithm for density-based clustering that takes as its premise the idea that clusters are clumps of data that are physically separated by less dense areas. Specifically, it creates a cluster out of ‘densely clustered’ data points. By inspecting the regional density of the data points, it can locate groups in massive spatial datasets. DBSCAN clustering’s intriguing characteristic is its resistance to outliers. Unlike K-Means, where we have to provide the number of centroids, it does not require us to know the number of clusters in advance.\nThe DBSCAN algorithm is described in detail below:\n\nDBSCAN generates its initial data point at random (non-visited points).\nThis point’s neighborhood is extracted using an epsilon distance.\nIf there are enough data points in this region, the clustering mechanism initiates, and the current data point becomes the first point in the newest cluster; otherwise, it is classified as noise and is subsequently visited.\nEvery other point within epsilon distance of the first point in the new cluster likewise joins it as a member of the same cluster. The steps taken to ensure that all previously added data points are also part of the same cluster are repeated for all newly added data points.\nThe preceding two procedures are continued until all cluster nodes are identified. All of the points in the cluster’s immediate neighborhood have been explored and categorized. When we’ve finished with the current cluster, we’ll move on to the next one by retrieving and processing a previously unvisited point. This process is carried out until all of the data points have been checked off as visited.\n\nAdvantages of DSCAN:\n\nDoesn’t need the number of clusters to be set up front.\nAble to tell when data is just noise while clustering.\nThe DBSCAN algorithm can find clusters that are any size and any shape.\n\nDisadvantages of DBSCAN:\n\nWhen clusters have different densities, the DBSCAN algorithm doesn’t work.\nFails if the dataset is a neck type.\n\nIn DBSCAN, the hyperparameters are Min points and epsilon.\n\nMin points: Min points ≥ dimensionality +1\n\nIf the set of data is more noisy, we use Min. Points are bigger because it’s easy to get rid of noisy ones.\n\nRadius (Epsilon): Elbow method\n\nWe figure out the distance between each data point and then sort distances from farthest to closest, and then draw a graph between distance and point index. From the graph,we choose the best distance (epsilon) where the graph shows a sharp rise.\n\n\nCode\n# we use nearestneighbors for calculating distance between points\nfrom sklearn.neighbors import NearestNeighbors\n\n# calculating distances\nneigh=NearestNeighbors(n_neighbors=2)\ndistance=neigh.fit(filename)\n# indices and distance values\ndistances,indices=distance.kneighbors(filename)\n# Now sorting the distance increasing order\nsorting_distances=np.sort(distances,axis=0)\n# sorted distances\nsorted_distances=sorting_distances[:,1]\n# plot between distance vs epsilon\nplt.plot(sorted_distances, color='lightblue')\nplt.xlabel(\"Distance\")\nplt.ylabel(\"Epsilon\")\nplt.show()\n\n\n\n\n\nEpsilon optimization If we look at the graph, we can see that at epsilon, there are 6 sharp rises, so we choose epsilon(radius) to be 6.\nUtilizing optimized hyperparameters when configuring DBSCAN. After determining the total number of clusters, we depict the corresponding point counts for each cluster in the section below. In the end, for eps=6 and min_samples = 2 the labels are predicted and DSCAN clusters are displayed.\nNUMBER OF POINTS ON EACH CLUSTER\n\n\nCode\nMyDBSCAN = DBSCAN(eps=6, min_samples=2)\nMyDBSCAN.fit_predict(filename)\n#count the number of points in each cluster\nfrom collections import Counter\nCounter(MyDBSCAN.labels_)\n#-1 means noise\n\n\nCounter({0: 3295, -1: 2})\n\n\n\n\nCode\n#plot the counter\nplt.bar(Counter(MyDBSCAN.labels_).keys(), Counter(MyDBSCAN.labels_).values())\n#labels for the graph\nplt.xlabel('Cluster')\nplt.ylabel('Number of points in cluster')\nplt.title('Distribution of points over clusters')\nplt.show()\n#-1 means noise\n\n\n\n\n\n\n\nCode\nfrom collections import Counter\nlabels_DB = MyDBSCAN.labels_\nCounter(labels_DB)\n#plot clusters using PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(filename)\nX_pca = pca.transform(filename)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_DB, s=50)\nplt.xlabel('clusters')\nplt.title('DBSCAN Clusters')\nplt.show()\n\n\n\n\n\n\n\nCode\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(filename, labels_DB))\n\n\nSilhouette Coefficient: -0.210\n\n\nHIERARCHIAL CLUSTERING\nIn order to construct layered clusters, hierarchical clustering algorithms repeatedly combine and divide existing clusters. As a tree, this structure depicts the relative hierarchy of the groups (or dendrogram). The single cluster that contains all of the others serves as the tree’s trunk, while the many other clusters that each contain exactly one sample serve as the tree’s leaves.\nAgglomerative hierarchical clustering (AHC):\nWith the AgglomerativeClustering object, you may do hierarchical clustering from the bottom up, meaning that each observation is placed in its own cluster before being combined with others. While AgglomerativeClustering can scale to a large number of samples when used in conjunction with a connectivity matrix, it incurs a high computational cost when no connection constraints are placed between samples since it considers all possible mergers at each step.\nAdvantages of AHC:\n\nAHC is easy to set up, and it can also arrange objects in a way that is helpful for the display.\nWe don’t have to know ahead of time how many clusters there will be. By cutting the dendrogram at a certain level, it’s easy to figure out how many clusters there are.\nIn the AHC method, smaller groups of data will be put together, which may show similarities.\n\nDisadvantages of AHC:\n\nIf you group the objects wrong in any of the first steps, you can’t go back and fix it.\nHierarchical clustering algorithms don’t give a unique way to divide the dataset, but they do give a hierarchy that can be used to choose which clusters to use.\nThey don’t do a good job with outliers. When outliers are found, they can lead to the formation of a new cluster or the merging of two or more clusters.\n\nThere are two key concepts in hierarchical clustering:\n\nThe bottom-up implementation of this algorithm is described above. Another option is to work from the top down, initially placing all data points in the same cluster before recursively splitting them into their own groups.\nClusters are merged based on how near they are to one another.\n\nHere,The Euclidean distance between the points is used to do aglomerative clustering for 4 clusters. Followed by prediction of labels and plotting dendrogram for the data.\nLABELS\n\n\nCode\n##  Hierarchical \n\nMyHC = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\nFIT=MyHC.fit(filename)\nHC_labels = MyHC.labels_\nprint(HC_labels)\n\n\n[2 2 2 ... 3 3 3]\n\n\n\n\nCode\n#plot hierarchical clusters\nplt.figure(figsize =(12, 12))\nplt.title('Hierarchical Clustering')\ndendro = hc.dendrogram((hc.linkage(filename, method ='ward')))\nplt.axhline(y=35000, color='r', linestyle='--')\nplt.xlabel('Index')\nplt.ylabel('Distance')\nplt.show()\n\n\n\n\n\nRESULTS\n\nFrom the Kmeans Algorithm, the dataset is 2 groups using the unsupervised k-means algorithm.\nFrom the DBSCAN Algorithm, the Silhouette Coefficient is 0.069, which means that the clusters are overlapping because the value is near 0.\nFrom Hierarchical Algorithm, the Euclidean distance method was used to make a dendrogram with k = 4. As can be seen, the clusters are very close to each other. It shows words that go together well, and similar words could be used in different tweets.The red dotted line indicates that the number of cluster is 2.\n\nCONCLUSION\nFrom the clusters formed for the #CRIME twitter data, we can say that  due to the enormous volume of the data and the possibility of similar words appearing in different tweets, the clusters are overlapping and not defined; however, this situation has room for further improvement."
  },
  {
    "objectID": "dt.html",
    "href": "dt.html",
    "title": "DECISION TREE FOR RECORD DATA",
    "section": "",
    "text": "INTRODUCTION\nThe decision tree technique will primarily use victim record data since it provides a highly effective structure within which to lay out options and analyze the potential repercussions of those options. They also assist you in developing a balanced picture of the risks and benefits of each conceivable course of action.\nABOUT THE DATA\nThe record data that Decision Tree used relates to the categories of violent crime that include homicide (including murder and non-negligent manslaughter), robbery, and serious assault. Violent crime also includes rape. Offenses classified as violent involve the actual use of, or the threat to use, physical force.\nOnly offenses for which a report has been filed are included in the data displayed on the Crime Data Explorer; it is not a comprehensive record of all crimes. Understanding the multiple factors that contribute to criminal behavior and the reporting of crimes in a community is essential before attempting to analyze the data. In the absence of these considerations, the data that are currently available may be deceptive. The size and density of the population, the economy, the unemployment rate, the policies regarding prosecution, the judiciary, and corrections, the administrative and investigative focus of law enforcement, how people feel about crime and the police, and the actual strength of the police force are all important factors to consider.\nThe dataset consists of three categories of information regarding victims of violent crimes: their age, gender and race together with the various types of crimes that they were victims of.\nIMPORT LIBRARIES\n\n\nCode\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nIMPORT DATA\n\n\nCode\n# Read cleaned and balanced data\nage_data = pd.read_csv('../../data/modified-data/cleaned_agedata_dt.csv')\nrace_data = pd.read_csv(\"../../data/modified-data/cleaned_racedata_dt.csv\")\ngender_data = pd.read_csv(\"../../data/modified-data/cleaned_genderdata_dt.csv\")\nstate_data = pd.read_csv('../../data/modified-data/cleaned_state_crime_record_data.csv')\n\n\nVICTIMS AGE DATA\n\n\nCode\n#HEAD OF DATA\nage_data.head()\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Offense\n      Under10\n      Under15\n      Under20\n      Under25\n      Under30\n      Under35\n      Under40\n      Under45\n      Under50\n      Under55\n      Under60\n      Under65\n      Over65\n      Unknown\n      Crime_Type\n    \n  \n  \n    \n      0\n      1\n      Robbery\n      812\n      3580\n      16183\n      20963\n      18514\n      16214\n      13432\n      10666\n      8668\n      7911\n      7234\n      5098\n      5705\n      1038\n      NotGrave\n    \n    \n      1\n      2\n      Embezzlement\n      10\n      15\n      125\n      349\n      457\n      590\n      680\n      600\n      611\n      619\n      581\n      494\n      1263\n      143\n      NotGrave\n    \n    \n      2\n      3\n      Burglary/Breaking & Entering\n      2299\n      2731\n      19470\n      48303\n      56547\n      57358\n      53607\n      46521\n      42398\n      41461\n      41581\n      35348\n      67410\n      7005\n      NotGrave\n    \n    \n      3\n      4\n      Destruction/Damage/Vandalism\n      2390\n      3255\n      52537\n      115237\n      128312\n      124755\n      114773\n      99933\n      88356\n      81557\n      77720\n      62329\n      99641\n      14281\n      NotGrave\n    \n    \n      4\n      5\n      Stolen Property Offenses\n      956\n      282\n      3414\n      7367\n      8856\n      8372\n      7776\n      6677\n      6268\n      5607\n      6085\n      4067\n      7195\n      10162\n      NotGrave\n    \n  \n\n\n\n\nVICTIMS GENDER DATA\n\n\nCode\n#HEAD OF DATA\ngender_data.head()\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Offense\n      Male\n      Female\n      category\n    \n  \n  \n    \n      0\n      1\n      Embezzlement\n      3391\n      3071\n      Male\n    \n    \n      1\n      2\n      Robbery\n      86466\n      49124\n      Male\n    \n    \n      2\n      3\n      Extortion/Blackmail\n      9020\n      3354\n      Male\n    \n    \n      3\n      4\n      Motor Vehicle Theft\n      320919\n      197047\n      Male\n    \n    \n      4\n      5\n      Counterfeiting/Forgery\n      28217\n      26558\n      Male\n    \n  \n\n\n\n\nVICTIMS RACE DATA\n\n\nCode\n#HEAD OF DATA\nrace_data.head()\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      White\n      Black_or_African_American\n      American_Indian_or_Alaska_Native\n      Asian\n      Native_Hawaiian_or_Other_Pacific_Islander\n      Unknown\n      category\n    \n  \n  \n    \n      0\n      1\n      664950\n      299579\n      10595\n      21483\n      1531\n      66938\n      White\n    \n    \n      1\n      2\n      459\n      60\n      9\n      8\n      0\n      35\n      White\n    \n    \n      2\n      3\n      41299\n      6681\n      380\n      1431\n      79\n      6298\n      White\n    \n    \n      3\n      4\n      344639\n      122261\n      4400\n      9595\n      1224\n      38540\n      White\n    \n    \n      4\n      5\n      126846\n      32509\n      3219\n      2211\n      365\n      11893\n      White\n    \n  \n\n\n\n\nVIOLENT CRIME CATEGORY CATEGORY\n\n\nCode\n#HEAD OF DATA\nstate_data.head()\n\n\n\n\n\n\n  \n    \n      \n      State\n      Year\n      Population\n      Property_Crime_Rate\n      Property_Burglary_Rate\n      Property_Larceny_Rate\n      Property_Motor_Rate\n      Violent_Crime_Rate\n      Violent_Assault_Rate\n      Violent_Murder_Rate\n      Violent_Rape_Rate\n      Violent_Robbery_Rate\n    \n  \n  \n    \n      0\n      Alabama\n      1960\n      3266740\n      1035.4\n      355.9\n      592.1\n      87.3\n      186.6\n      138.1\n      12.4\n      8.6\n      27.5\n    \n    \n      1\n      Alabama\n      1961\n      3302000\n      985.5\n      339.3\n      569.4\n      76.8\n      168.5\n      128.9\n      12.9\n      7.6\n      19.1\n    \n    \n      2\n      Alabama\n      1962\n      3358000\n      1067.0\n      349.1\n      634.5\n      83.4\n      157.3\n      119.0\n      9.4\n      6.5\n      22.5\n    \n    \n      3\n      Alabama\n      1963\n      3347000\n      1150.9\n      376.9\n      683.4\n      90.6\n      182.7\n      142.1\n      10.2\n      5.7\n      24.7\n    \n    \n      4\n      Alabama\n      1964\n      3407000\n      1358.7\n      466.6\n      784.1\n      108.0\n      213.1\n      163.0\n      9.3\n      11.7\n      29.1\n    \n  \n\n\n\n\nDATA CLEANING\nFor the above datasets, columns that aren’t necessary are deleted, and it is determined whether or not the dataset contains any values that are not applicable or duplicates.\n\n\nCode\n#CLEAN THE DATA\n#AGE DATA\n#REMOVE THE FIRST COLUMN \nage_data = age_data.drop(columns=['Unnamed: 0', 'Offense'])\n#CHECK FOR MISSING VALUES\nage_null_count = age_data.isnull().sum()\n#GENDER DATA\n#REMOVE THE FIRST COLUMN \ngender_data = gender_data.drop(columns=['Unnamed: 0','Offense'])\n#CHECK FOR MISSING VALUES\ngender_null_count = gender_data.isnull().sum()\n#RACE DATA\n#REMOVE THE FIRST COLUMN\nrace_data = race_data.drop(columns=['Unnamed: 0'])\n#CHECK FOR MISSING VALUES\nrace_null_count = race_data.isnull().sum()\n#STATE DATA\n#CHECK FOR MISSING VALUES\nstate_null_count = state_data.isnull().sum()\n\n\nDATA SUMMARY:\nIn Python, the describe function can be used to get descriptive or summary statistics (). Describe The function gives the mean, standard deviation, IQR, minimum value, maximum value, mode and so on.\nVICTIM AGE DATA SUMMARY\n\n\nCode\n#SUMMARIZE THE AGE DATA\n\nage_data_describe = age_data.describe().loc[['min','mean','max']]\nage_data_dtype = age_data.dtypes\nage_data_describe = age_data_describe.append(age_data_dtype,ignore_index=True)\nage_data_describe = age_data_describe.rename(index={0:'min',1:'mean',2:'max',3:'dtype'})\nage_data_describe = age_data_describe.transpose()\nprint(age_data_describe)\n\n\n               min           mean       max   dtype\nUnder10       10.0   21119.529412   81158.0   int64\nUnder15       15.0        39137.0  143951.0   int64\nUnder20      125.0  102390.764706  256062.0   int64\nUnder25      349.0  174280.529412  351830.0   int64\nUnder30      457.0  188563.294118  374771.0   int64\nUnder35      590.0  177382.941176  347661.0   int64\nUnder40      680.0  154985.470588  292157.0   int64\nUnder45      600.0  126095.882353  224222.0   int64\nUnder50      611.0  106786.352941  175238.0   int64\nUnder55      619.0        94931.0  163137.0   int64\nUnder60      581.0   86457.529412  154999.0   int64\nUnder65      494.0   65247.352941  125770.0   int64\nOver65      1263.0  102299.470588  222164.0   int64\nUnknown      143.0   18377.823529   32956.0   int64\nCrime_Type     NaN            NaN       NaN  object\n\n\nVICTIM GENDER DATA SUMMARY\n\n\nCode\n#SUMMARIZE THE GENDER DATA \ndf_describe = gender_data.describe().loc[['min','mean','max']]\ndf_dtype = gender_data.dtypes\ndf_describe = df_describe.append(df_dtype,ignore_index=True)\ndf_describe = df_describe.rename(index={0:'min',1:'mean',2:'max',3:'dtype'})\ndf_describe = df_describe.transpose()\nprint(df_describe)\n\n\n             min      mean       max   dtype\nMale       335.0  138022.5  508037.0   int64\nFemale    1659.0  119444.0  549220.0   int64\ncategory     NaN       NaN       NaN  object\n\n\nVICTIM RACE DATA SUMMARY\n\n\nCode\n#SUMMARIZE THE RACE DATA  \ndf_describe = race_data.describe().loc[['min','mean','max']]\ndf_dtype = race_data.dtypes\ndf_describe = df_describe.append(df_dtype,ignore_index=True)\ndf_describe = df_describe.rename(index={0:'min',1:'mean',2:'max',3:'dtype'})\ndf_describe = df_describe.transpose()\nprint(df_describe)\n\n\n                                             min            mean        max  \\\nWhite                                      459.0  1509511.888889  5372573.0   \nBlack_or_African_American                   60.0   648793.833333  2088414.0   \nAmerican_Indian_or_Alaska_Native             9.0         27830.5    86370.0   \nAsian                                        8.0    42430.166667   166046.0   \nNative_Hawaiian_or_Other_Pacific_Islander    0.0     5009.888889    16887.0   \nUnknown                                     35.0   134815.111111   537555.0   \ncategory                                     NaN             NaN        NaN   \n\n                                            dtype  \nWhite                                       int64  \nBlack_or_African_American                   int64  \nAmerican_Indian_or_Alaska_Native            int64  \nAsian                                       int64  \nNative_Hawaiian_or_Other_Pacific_Islander   int64  \nUnknown                                     int64  \ncategory                                   object  \n\n\nCRIME CATEGORY DATA SUMMARY\n\n\nCode\n\n#SUMMARIZE THE VIOLENT CRIME DATA\nstate_data_describe = state_data.describe().loc[['min','mean','max']]\nstate_data_dtype = state_data.dtypes\nstate_data_describe = state_data_describe.append(state_data_dtype,ignore_index=True)\nstate_data_describe = state_data_describe.rename(index={0:'min',1:'mean',2:'max',3:'dtype'})\nstate_data_describe = state_data_describe.transpose()\nprint(state_data_describe)\n\n\n                             min            mean          max    dtype\nYear                      1960.0     1989.544141       2019.0    int64\nPopulation              226167.0  9708501.690209  328239523.0    int64\nProperty_Crime_Rate        573.1     3542.202311       9512.1  float64\nProperty_Burglary_Rate     126.3       876.53252       2906.7  float64\nProperty_Larceny_Rate      293.3     2322.659133       5833.8  float64\nProperty_Motor_Rate         28.4        343.0113       1839.9  float64\nViolent_Crime_Rate           9.5      397.877047       2921.8  float64\nViolent_Assault_Rate         3.6       237.36504       1557.6  float64\nViolent_Murder_Rate          0.2        6.477207         80.6  float64\nViolent_Rape_Rate            0.8       30.179872        161.6  float64\nViolent_Robbery_Rate         1.9      123.853258       1635.1  float64\nState                        NaN             NaN          NaN   object\n\n\nDECISION TREE\nA decision tree is an aid to decision making that employs a tree-like model of decisions and the potential implications, such as the outcomes of random events, the costs and benefits of resources, and the overall value of the decision. If your method consists only of if/then statements, this is one approach to present it.\nAs a prominent tool in machine learning, decision trees are also widely used in the field of operations research, particularly in the field of decision analysis, to determine which course of action is most likely to result in the desired outcome.\nEach node inside a decision tree represents a “test” on an attribute (such as whether a coin is headed up or down), each branch reflects the result of that test, and each leaf node represents a class label (decision taken after computing all attributes). The branches stand for different kinds of categorization schemes.\nSplit the dataset into training and testing sets\nThe data is separated into training data and testing data, and each of the three datasets contains unique information. Before continuing, the number of samples that will be used for each sample will be calculated.\nHYPER-PARAMETERS TUNING\nThe features of the model known as parameters are those that are learned by the model from the data. On the other hand, hyperparameters are arguments that are accepted by a model-making function. These hyperparameters can be adjusted to reduce overfitting, which ultimately results in a model that is more generalizable. The method of hyperparameter tuning, which involves calibrating our model by determining which hyperparameters should be used to extend our model, has been given its own name.\nFOR VICTIM AGE DATA\nDATA NORMAIZATION\n\n\nCode\n#THE LOAD BALANCE AND COUNT THE NUMBER OF SAMPLES FOR EACH CATEGORY\nfrom locale import normalize\n\nCrime_Type_Grave_count = age_data['Crime_Type'].value_counts()['Grave']\nCrime_Type_NotGrave_count = age_data['Crime_Type'].value_counts()['NotGrave']\nCrime_Type_Grave_norm = age_data['Crime_Type'].value_counts(normalize = True)['Grave']\nCrime_Type_NotGrave_norm = age_data['Crime_Type'].value_counts(normalize = True)['NotGrave']\n\nprint(\"Number of points with category Grave: {0:2d} {1:}\".format(Crime_Type_Grave_count, Crime_Type_Grave_norm))\nprint(\"Number of points with category Not Grave: {0:2d} {1:}\".format(Crime_Type_NotGrave_count, Crime_Type_NotGrave_norm))\n\n\nNumber of points with category Grave:  9 0.5294117647058824\nNumber of points with category Not Grave:  8 0.47058823529411764\n\n\nSPLITTING DATA INTO TRAINING AND TESTING DATA IN SKLEARN\n\n\nCode\n#MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"category\" COLUMN and X=\"everything else\"\nX = age_data.drop(columns = ['Crime_Type'])\nY = age_data['Crime_Type']\n#PARTITION THE DATASET INTO TRAINING AND TEST SETS\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.62, random_state=2)\n\n#CONSISTENCY CHECK\nprint(type(x_train))\nprint(\"X train shape: \",x_train.shape)\nprint(type(y_train))\nprint(\"Y train shape: \",y_train.shape)\nprint(type(x_test))\nprint(\"X test shape: \",x_test.shape)\nprint(type(y_test))\nprint(\"Y test shape\",y_test.shape)\n\n\n<class 'pandas.core.frame.DataFrame'>\nX train shape:  (6, 14)\n<class 'pandas.core.series.Series'>\nY train shape:  (6,)\n<class 'pandas.core.frame.DataFrame'>\nX test shape:  (11, 14)\n<class 'pandas.core.series.Series'>\nY test shape (11,)\n\n\nDECISION TREE MODEL FOR VICTIM AGE DATA\n\n\nCode\n#set seed\nnp.random.seed(2)\n# TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(x_train, y_train)\n\n#MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\nCONFUSION MATRIX FOR VICTIM AGE MODEL\n\n\nCode\n#CONFUSION MATRIX \nfrom sklearn.metrics import confusion_matrix\n\n\ndef confusion_plot(y_data,y_pred):\n    cm = confusion_matrix(y_data, y_pred)\n    print('ACCURACY: {:.2f}'.format(accuracy_score(y_data, y_pred)))\n    print('NEGATIVE RECALL (Y=0): {:.2f}'.format(recall_score(y_data, y_pred, pos_label='Grave')))\n    print('NEGATIVE PRECISION (Y=0): {:.2f}'.format(precision_score(y_data, y_pred, pos_label='Grave')))\n    print('POSITIVE RECALL (Y=1): {:.2f}'.format(recall_score(y_data, y_pred, pos_label='Grave')))\n    print('POSITIVE PRECISION (Y=1): {:.2f}'.format(precision_score(y_data, y_pred, pos_label='NotGrave')))\n    print(cm)\n    plt.figure(figsize=(10,10))\n    sns.heatmap(cm, annot=True, fmt=\"d\", )\n    plt.title('Confusion matrix')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n#TEST SET CONFUSION MATRIX\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n\n------TEST------\nACCURACY: 0.73\nNEGATIVE RECALL (Y=0): 1.00\nNEGATIVE PRECISION (Y=0): 0.57\nPOSITIVE RECALL (Y=1): 1.00\nPOSITIVE PRECISION (Y=1): 1.00\n[[4 0]\n [3 4]]\n\n\n\n\n\nDECISION TREE VISUALIZATION\n\n\nCode\n# VISUALIZE THE DECISION TREE \ndef plot_tree(model,X,Y):\n    plt.figure(figsize=(10,10))\n    tree.plot_tree(model, feature_names=X.columns, class_names=Y.name, filled=True)\n    plt.show()\n\nplot_tree(model,X,Y)\n\n\n\n\n\nHYPER-PARAMETERS TUNING\n\n\nCode\n#set seed\nnp.random.seed(6)\n#HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,17):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label='Grave'),recall_score(y_test, yp_test,pos_label='NotGrave')])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label='Grave'),recall_score(y_train, yp_train,pos_label='NotGrave')])\n\n\n\n\n\nCode\n#### TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=18)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n# THE MODEL ON THE TEST SET\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\nplot_tree(model,X,Y)\n\n\n------TEST------\nACCURACY: 0.82\nNEGATIVE RECALL (Y=0): 1.00\nNEGATIVE PRECISION (Y=0): 0.67\nPOSITIVE RECALL (Y=1): 1.00\nPOSITIVE PRECISION (Y=1): 1.00\n[[4 0]\n [2 5]]\n\n\n\n\n\n\n\n\nINFERENCE FOR VICTIM AGE DECISION MODEL: - The split ratio of teh model is 62% of training data and rest as testinf data, it is categoried with crime type variable(grave and not grave). - The accurary of the model before hyper-parametric tuning is 73%. - The accuracy of the model after hyper parametric tuning has slightly increased to 82%. - The model is not underfitting as accuracy is greater than 50%. - The decision tree visulization is about the Age over 65 and Unknown Age of the data, which classifies into different samples. As the sample of the data is very small and the data category is less, the decision tree doesn’t have huge classification if the data. - As the max_depth increases, the accuracy increases and the optimal tree becomes better.\nNOTE: Since the dataset is small and the accuracy of the model is high, the graph for hyperparametric isn’t required as the graph shows similar results for training and testing data.\nFOR VICTIM GENDER DATA\nDATA NORMAIZATION\n\n\nCode\n#THE LOAD BALANCE AND COUNT THE NUMBER OF SAMPLES FOR EACH CATEGORY\nfrom locale import normalize\n\ncategory_male_count = gender_data['category'].value_counts()['Male']\ncategory_female_count = gender_data['category'].value_counts()['Female']\ncategory_male_norm = gender_data['category'].value_counts(normalize = True)['Male']\ncategory_female_norm = gender_data['category'].value_counts(normalize = True)['Female']\n\nprint(\"Number of points with category white: {0:2d} {1:}\".format(category_male_count, category_male_norm))\nprint(\"Number of points with category black: {0:2d} {1:}\".format(category_female_count, category_female_norm))\n\n\nNumber of points with category white: 10 0.625\nNumber of points with category black:  6 0.375\n\n\nSPLITTING DATA INTO TRAINING AND TESTING DATA IN SKLEARN\n\n\nCode\n#MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"category\" COLUMN and X=\"everything else\"\nX = gender_data.drop(columns = ['category'])\nY = gender_data['category']\n#PARTITION THE DATASET INTO TRAINING AND TEST SETS\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=.52, random_state=2)\n\n#CONSISTENCY CHECK\nprint(type(x_train))\nprint(\"X train shape: \",x_train.shape)\nprint(type(y_train))\nprint(\"Y train shape: \",y_train.shape)\nprint(type(x_test))\nprint(\"X test shape: \",x_test.shape)\nprint(type(y_test))\nprint(\"Y test shape\",y_test.shape)\n\n\n<class 'pandas.core.frame.DataFrame'>\nX train shape:  (8, 2)\n<class 'pandas.core.series.Series'>\nY train shape:  (8,)\n<class 'pandas.core.frame.DataFrame'>\nX test shape:  (8, 2)\n<class 'pandas.core.series.Series'>\nY test shape (8,)\n\n\nDECISION TREE MODEL FOR VICTIM GENDER DATA\n\n\nCode\n#set seed\nnp.random.seed(2)\n# TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(x_train, y_train)\n#MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\nCONFUSION MATRIX FOR VICTIM GENDER MODEL\n\n\nCode\n#CONFUSION MATRIX \nfrom sklearn.metrics import confusion_matrix\n\n\ndef confusion_plot(y_data,y_pred):\n    cm = confusion_matrix(y_data, y_pred)\n    print('ACCURACY: {:.2f}'.format(accuracy_score(y_data, y_pred)))\n    print('NEGATIVE RECALL (Y=0): {:.2f}'.format(recall_score(y_data, y_pred, pos_label='Male')))\n    print('NEGATIVE PRECISION (Y=0): {:.2f}'.format(precision_score(y_data, y_pred, pos_label='Male')))\n    print('POSITIVE RECALL (Y=1): {:.2f}'.format(recall_score(y_data, y_pred, pos_label='Male')))\n    print('POSITIVE PRECISION (Y=1): {:.2f}'.format(precision_score(y_data, y_pred, pos_label='Female')))\n    print(cm)\n    plt.figure(figsize=(10,10))\n    sns.heatmap(cm, annot=True, fmt=\"d\", )\n    plt.title('Confusion matrix')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n#TEST SET CONFUSION MATRIX\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n\n------TEST------\nACCURACY: 0.88\nNEGATIVE RECALL (Y=0): 0.83\nNEGATIVE PRECISION (Y=0): 1.00\nPOSITIVE RECALL (Y=1): 0.83\nPOSITIVE PRECISION (Y=1): 0.67\n[[2 0]\n [1 5]]\n\n\n\n\n\nDECISION TREE VISUALIZATION\n\n\nCode\n# VISUALIZE THE DECISION TREE \ndef plot_tree(model,X,Y):\n    plt.figure(figsize=(10,10))\n    tree.plot_tree(model, feature_names=X.columns, class_names=Y.name, filled=True)\n    plt.show()\n\nplot_tree(model,X,Y)\n\n\n\n\n\nHYPER-PARAMETERS TUNING\n\n\nCode\n#set seed\nnp.random.seed(676)\n#HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,17):\n    model = tree.DecisionTreeClassifier(max_depth=18)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label='Male'),recall_score(y_test, yp_test,pos_label='Female')])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label='Male'),recall_score(y_train, yp_train,pos_label='Female')])\n\n\n\n\n\nCode\n#### TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=18)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n# THE MODEL ON THE TEST SET\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\nplot_tree(model,X,Y)\n\n\n------TEST------\nACCURACY: 0.88\nNEGATIVE RECALL (Y=0): 0.83\nNEGATIVE PRECISION (Y=0): 1.00\nPOSITIVE RECALL (Y=1): 0.83\nPOSITIVE PRECISION (Y=1): 0.67\n[[2 0]\n [1 5]]\n\n\n\n\n\n\n\n\nINFERENCE FOR VICTIM GENDER DECISION MODEL: - The split ratio of teh model is 52% of training data and rest as testinf data, it is categoried with category variable(male and female). - The accurary of the model before and after hyper-parametric tuning is 88%. - The model is not underfitting as accuracy is greater than 50%. - As the max_depth increases, the accuracy increases and the optimal tree becomes better. - The decision tree visulization is about the Male and Female category of the data, which classifies into different samples.\nNOTE: Since the dataset is small and the accuracy of the model is high, the graph for hyperparametric isn’t required as the graph shows similar results for training and testing data.\nFOR VICTIM RACE DATA\nDATA NORMALIZATION\n\n\nCode\n#THE LOAD BALANCE AND COUNT THE NUMBER OF SAMPLES FOR EACH CATEGORY\nfrom locale import normalize\n\ncategory_white_count = race_data['category'].value_counts()['White']\ncategory_black_count = race_data['category'].value_counts()['Black_or_African_American']\ncategory_white_norm = race_data['category'].value_counts(normalize = True)['White']\ncategory_black_norm = race_data['category'].value_counts(normalize = True)['Black_or_African_American']\n\nprint(\"Number of points with category white: {0:2d} {1:}\".format(category_white_count, category_white_norm))\nprint(\"Number of points with category black: {0:2d} {1:}\".format(category_black_count, category_black_norm))\n\n\nNumber of points with category white:  8 0.4444444444444444\nNumber of points with category black: 10 0.5555555555555556\n\n\nSPLITTING DATA INTO TRAINING AND TESTING DATA IN SKLEARN\n\n\nCode\n#MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"category\" COLUMN and X=\"everything else\"\nX = race_data.drop(columns = ['category'])\nY = race_data['category']\n#PARTITION THE DATASET INTO TRAINING AND TEST SETS\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=.52, random_state=2)\n\n#CONSISTENCY CHECK\nprint(type(x_train))\nprint(\"X train shape: \",x_train.shape)\nprint(type(y_train))\nprint(\"Y train shape: \",y_train.shape)\nprint(type(x_test))\nprint(\"X test shape: \",x_test.shape)\nprint(type(y_test))\nprint(\"Y test shape\",y_test.shape)\n\n\n<class 'pandas.core.frame.DataFrame'>\nX train shape:  (9, 6)\n<class 'pandas.core.series.Series'>\nY train shape:  (9,)\n<class 'pandas.core.frame.DataFrame'>\nX test shape:  (9, 6)\n<class 'pandas.core.series.Series'>\nY test shape (9,)\n\n\nDECISION TREE MODEL FOR VICTIM RACE DATA\n\n\nCode\n#set seed\nnp.random.seed(2)\n# TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(x_train, y_train)\n#MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\nCONFUSION M ATRIX FOR VICTIM RACE MODEL\n\n\nCode\n#CONFUSION MATRIX \nfrom sklearn.metrics import confusion_matrix\n\n\ndef confusion_plot(y_data,y_pred):\n    cm = confusion_matrix(y_data, y_pred)\n    print('ACCURACY: {:.2f}'.format(accuracy_score(y_data, y_pred)))\n    print('NEGATIVE RECALL (Y=0): {:.2f}'.format(recall_score(y_data, y_pred, pos_label='White')))\n    print('NEGATIVE PRECISION (Y=0): {:.2f}'.format(precision_score(y_data, y_pred, pos_label='White')))\n    print('POSITIVE RECALL (Y=1): {:.2f}'.format(recall_score(y_data, y_pred, pos_label='White')))\n    print('POSITIVE PRECISION (Y=1): {:.2f}'.format(precision_score(y_data, y_pred, pos_label='Black_or_African_American')))\n    print(cm)\n    plt.figure(figsize=(10,10))\n    sns.heatmap(cm, annot=True, fmt=\"d\", )\n    plt.title('Confusion matrix')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n#TEST SET CONFUSION MATRIX\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n\n------TEST------\nACCURACY: 0.89\nNEGATIVE RECALL (Y=0): 0.80\nNEGATIVE PRECISION (Y=0): 1.00\nPOSITIVE RECALL (Y=1): 0.80\nPOSITIVE PRECISION (Y=1): 0.80\n[[4 0]\n [1 4]]\n\n\n\n\n\n\n\nCode\n# VISUALIZE THE DECISION TREE \ndef plot_tree(model,X,Y):\n    plt.figure(figsize=(10,10))\n    tree.plot_tree(model, feature_names=X.columns, class_names=Y.name, filled=True)\n    plt.show()\n\nplot_tree(model,X,Y)\n\n\n\n\n\nHYPER-PARAMETERS TUNING\n\n\nCode\n#set seed\nnp.random.seed(67)\n#HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,17):\n    model = tree.DecisionTreeClassifier(max_depth=12)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label='White'),recall_score(y_test, yp_test,pos_label='Black_or_African_American')])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label='White'),recall_score(y_train, yp_train,pos_label='Black_or_African_American')])\n\n\n\n\n\nCode\n#### TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=50)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n# THE MODEL ON THE TEST SET\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\nplot_tree(model,X,Y)\n\n\n------TEST------\nACCURACY: 0.89\nNEGATIVE RECALL (Y=0): 0.80\nNEGATIVE PRECISION (Y=0): 1.00\nPOSITIVE RECALL (Y=1): 0.80\nPOSITIVE PRECISION (Y=1): 0.80\n[[4 0]\n [1 4]]\n\n\n\n\n\n\n\n\nINFERENCE FOR VICTIM RACE DECISION MODEL: - The split ratio of teh model is 52% of training data and rest as testinf data, it is categoried with race category variable. - The accurary of the model before and after hyper-parametric tuning is 89%. - The model is not underfitting as accuracy is greater than 50%. - As the max_depth increases, the accuracy increases and the optimal tree becomes better. - The decision tree visulization is about White and Pacific Islander category of the data, which classifies into different samples.\nNOTE: Since the dataset is small and the accuracy of the model is high, the graph for hyperparametric isn’t required as the graph shows similar results for training and testing data.\nFOR VIOLENT CRIME CATEGORY DATA\nDATA NORMLIZATION\n\n\nCode\n#create new column with the predicted category\nstate_data['Violent_label'] = state_data[['Violent_Assault_Rate', 'Violent_Murder_Rate','Violent_Rape_Rate','Violent_Robbery_Rate']].idxmax(axis=1)\n#create table for only property crime datasets\nviolent_df = state_data[['Violent_Assault_Rate', 'Violent_Murder_Rate','Violent_Rape_Rate','Violent_Robbery_Rate','Violent_label']].copy()\n\n#THE LOAD BALANCE AND COUNT THE NUMBER OF SAMPLES FOR EACH CATEGORY\nfrom locale import normalize\n\nViolent_Assault_Rate_count = violent_df['Violent_label'].value_counts()['Violent_Assault_Rate']\nViolent_Robbery_Rate_count = violent_df['Violent_label'].value_counts()['Violent_Robbery_Rate']\nViolent_Rape_Rate_count = violent_df['Violent_label'].value_counts()['Violent_Rape_Rate']\nViolent_Rape_Rate_norm = violent_df['Violent_label'].value_counts(normalize = True)['Violent_Rape_Rate']\nViolent_Assault_Rate_norm = violent_df['Violent_label'].value_counts(normalize = True)['Violent_Assault_Rate']\nViolent_Robbery_Rate_norm = violent_df['Violent_label'].value_counts(normalize = True)['Violent_Robbery_Rate']\nprint(\"Number of points with category Assualt: {0:2d} {1:}\".format(Violent_Assault_Rate_count, Violent_Assault_Rate_norm))\nprint(\"Number of points with category Robbery: {0:2d} {1:}\".format(Violent_Robbery_Rate_count, Violent_Robbery_Rate_norm))\nprint(\"Number of points with category Rape: {0:2d} {1:}\".format(Violent_Rape_Rate_count, Violent_Rape_Rate_norm))\n\n\nNumber of points with category Assualt: 2825 0.9069020866773676\nNumber of points with category Robbery: 287 0.09213483146067415\nNumber of points with category Rape:  3 0.0009630818619582664\n\n\nSPLITTING DATA INTO TRAINING AND TESTING DATA IN SKLEARN\n\n\nCode\n#MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"category\" COLUMN and X=\"everything else\"\nX = violent_df.drop(columns = ['Violent_label'])\nY = violent_df['Violent_label']\n#PARTITION THE DATASET INTO TRAINING AND TEST SETS\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=2)\n\n#CONSISTENCY CHECK\nprint(type(x_train))\nprint(\"X train shape: \",x_train.shape)\nprint(type(y_train))\nprint(\"Y train shape: \",y_train.shape)\nprint(type(x_test))\nprint(\"X test shape: \",x_test.shape)\nprint(type(y_test))\nprint(\"Y test shape\",y_test.shape)\n\n\n<class 'pandas.core.frame.DataFrame'>\nX train shape:  (623, 4)\n<class 'pandas.core.series.Series'>\nY train shape:  (623,)\n<class 'pandas.core.frame.DataFrame'>\nX test shape:  (2492, 4)\n<class 'pandas.core.series.Series'>\nY test shape (2492,)\n\n\nDECISION TREE MODEL FOR VIOLENT CRIME CATEGORY DATA\n\n\nCode\n#set seed\nnp.random.seed(2)\n# TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(x_train, y_train)\n#MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\nCONFUSION MATRIX FOR VIOLENT CATEGORY MODEL\n\n\nCode\nprint(\"------TEST------\")\nprint('ACCURACY: {:.2f}'.format(accuracy_score(y_test, yp_test)))\n##Visualise Confusion Matrix\nplt.figure(figsize=(10,10))\nlabels = ['Assault','Robbery','Rape']\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(y_test, yp_test), annot=True, fmt='g', ax=ax1)\n# labels, title and ticks\nax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels')\nax1.set_title('Confusion Matrix'); \nax1.xaxis.set_ticklabels(labels); ax1.yaxis.set_ticklabels(labels)\nplt.show()\nplt.close()\n\n\n------TEST------\nACCURACY: 0.96\n\n\n\n\n\nDECISION TREE VISUALIZATION\n\n\nCode\n# VISUALIZE THE DECISION TREE \ndef plot_tree(model,X,Y):\n    plt.figure(figsize=(15,15))\n    tree.plot_tree(model, feature_names=X.columns, class_names=Y.name, filled=True)\n    plt.show()\n\nplot_tree(model,X,Y)\n\n\n\n\n\nHYPER-PARAMETERS TUNING\n\n\nCode\n#set seed\nnp.random.seed(6)\n#HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,17):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n\n\n\nCode\n#### TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=18)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n# THE MODEL ON THE TEST SET\nprint(\"------TEST------\")\nprint('ACCURACY: {:.2f}'.format(accuracy_score(y_test, yp_test)))\n##Visualise Confusion Matrix\nplt.figure(figsize=(10,10))\nlabels = ['Assault','Robbery','Rape']\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(y_test, yp_test), annot=True, fmt='g', ax=ax1)\n# labels, title and ticks\nax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels')\nax1.set_title('Confusion Matrix'); \nax1.xaxis.set_ticklabels(labels); ax1.yaxis.set_ticklabels(labels)\nplt.show()\nplt.close()\nplot_tree(model,X,Y)\n\n\n------TEST------\nACCURACY: 0.96\n\n\n\n\n\n\n\n\nINFERENCE FOR VIOLENT CRIME CATEGORY DECISION MODEL: - The split ratio of teh model is 80% of training data and rest as testing data, it is categoried with violent category variable. - The accurary of the model before and after hyper-parametric tuning is 97%. - The model is not underfitting as accuracy is greater than 50%. - As the max_depth increases, the accuracy increases and the optimal tree becomes better. - The decision tree visulization are classified into different samples, accoring to the murder, robbery and rape violent crime category.\nCONCLUSION\nThe purpose of this Decision Tree study was to categorize the age, race, and gender category based on the sorts of violent crimes committed. After hyper-parametric tuning, the model achieves an impressively high accuracy of 82% in predicting age, 88% in predicting gender, and 88% in predicting race. The tuning graph is not displayed above because the model performs similarly on both the training and testing data, which is limited in size. When looking at the Violent category model, since the data is of large size, the accuracy of the models 97% and the decision tree gives us a lot of explanantion when compared to the other datasets used."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "CONCLUSION",
    "section": "",
    "text": "Crime is something we want to eradicate, but haven’t been able to. The purpose was to study the crime, crime rate and who is affected by the crime rate and how they are affected. The crime rate has decreased compared to 1900s, although we witness spike in crime rate post covid. I concluded that adults aged 25 and older have been victimized by the crime more frequently than younger individuals. I attempted to evaluate the elements that could be associated with the crimes and how victims could be selected. I collected the data for crime in the United States for crime type, age groupings of victims, gender, race. The second dataset contains information about offenders over time.\nAccording to the raw data, Texas has the highest crime rate in the United States, with the majority of crimes occurring within the home. Assault crimes have been registered at a higher rate. A breach of drug abuse laws has also been committed.\nText data was collected on major crime hastags to understand people’s opinion on the ongoing crime in United States and also to understand what kind of crime people are having to face on everyday basis.\n\nNaive Bayes for Text Data\nThe Naive Bayes algorithm was applied to text data to classify tweets based on the FBI and USACrime hashtags, which were used to collect data and determine the significance of crime in the United States. The model was trained to categorize these tweets into their appropriate categories. This model achieved an accuracy of 96%, which is pretty accurate.\n\n\nSVM on Text Data\nSVM was applied on the text data with the same goal as Naive Bayes to classify the tweets, The models have an accuracy ranging from 96% to 99% for determining the category, which is considered to be quite high. The collection of terms enables the model to make predictions about tweets or classify them into distinct groups. Since they account for 99.9% of the variation, the linear model and RBF Kernel model provide the most accurate depictions of these data.\n\n\nClustering on Text data\nClustering was performed on text data to discover similarities between the texts in tweets including the hashtags for CRIME; the resulting clusters were overlapping and thick, possibly due to the massive volume of data and the probability of similar phrases appearing in various tweets.\n\n\nNaive Bayes on record data\nHomicide (murder and non-negligent manslaughter), rape, robbery, and major assault constitute violent crime, according to the records used by Naive Bayes. Violent crimes employ or threaten the use of physical force.\nThe data on the Crime Data Explorer only includes recorded incidents; it is not a comprehensive database of all crimes. Before analyzing the data, it is essential to comprehend the several causes of criminal behavior and crime reporting in a community. Existing data may be misleading in the absence of certain components. There are numerous factors to consider, including population size and density, the economy, the unemployment rate, the prosecutorial, judicial, and correctional policies, the administrative and investigative focus of law enforcement, how people feel about crime and police, and the actual strength of the police force.\nThe dataset includes three sorts of information regarding violent crime victims: their age, gender, and race, as well as the type of crime committed.\nThe state’s crime rate statistics include property and violent crime rates. For naive bayes, the violent crime rate is evaluated, which is further subdivided into categories such as murder, rape, robbery, and assault crime. The second set of data is comprised of hate crime statistics from 1990 through 2020.\nThe purpose of the naive Bayes study was to enhance the accuracy of victim age, victim race, victim gender, violent crime rate, and hate crime forecasts for each category, given the types of crimes committed. The model’s accuracy in predicting the relationship between age group and hate crimes is 60%, which is above average for this age group. The model’s accuracy in predicting gender and race is 75%, which is a reasonable accuracy for that gender and race group, however the model’s accuracy in forecasting violent crime from 1960 to 2019 is 93%, which is a rather high accuracy.\n\n\nDecision Tree on Record data\nDecision tree was applied on the same data. The objective of this Decision Tree study was to classify age, race, and gender according to the types of violent crimes committed. After hyper-parametric tweaking, the model predicts age with an impressively high accuracy of 82%, gender with an accuracy of 88%, and race with an accuracy of 88%. The tuning graph is not shown because the model performs equally on both training and testing data, which are of modest size. The accuracy of the Violent category model is 97% due to the amount of the data, and the decision tree provides a great deal of explanation in comparison to the other datasets used.\n\n\nClustering on Record Data\nThis information is obtained from an FBI source that tracks state-by-state crime statistics from 1960 to 2019. The data contains the property crime rate and the violent crime rate. However, we will focus primarily on the rate of violent crime because that was the basis for the decision tree analysis. Assault, rape, robbery, and murder are the four different sorts of violent crimes. The aim is to look for connections between the crimes. Clustering the dataset provided insights into that. The data clusters are overlapping and could be improved.\n\n\nARM and Networking on Record Data\nThe primary objective is to find a quick way to summarize state record data from 1990 to 2019 using Networks. Networks provide a theoretical framework for conceptually representing interrelations in a wide variety of systems and discovering statistically significant interrelationships between variables in large databases. From the network graph, I could determine how different types of crimes and years are related with specific locations. When considering the two primary crime categories, the graphs indicate that there is a correlation with two distinct crime types, indicating that there is an association between each crime category.\n\n\nConclusion\nIt is undeniable that the rate of crime is increasing in all cultures around the world, but I personally feel that there is plenty that governments and individuals can do to reduce crime in communities.\nOn the one hand, governments can take a variety of significant steps to minimize or even eliminate various sorts of crime.\nFirst, governments can deploy more police officers throughout the country to monitor people’s actions and prevent them from committing crimes. Second, the government can use new technologies like surveillance cameras in the streets, shopping malls, restaurants, and other public locations to deter thieves. Third, heavy punishments for offenders can have significant preventative and deterrent effects on all age groups in society, therefore by employing harsh penalties such as imprisonment, physical or financial punishments, the rate of crime can be reduced.\nIndividuals in society, on the other hand, can be of considerable assistance in reducing the amount of crimes committed. To my perspective, the vast majority of individuals tend to participate in activities that help the government keep society secure for their own families, others, and people of all ages. As an example, most people may play an important role in crime prevention by reporting problems to police. Furthermore, when citizens are concerned about reducing horrific crimes in cities, government can take preventive measures to improve the condition in society. To summarize, in order for a society to be a safe place to live, all members of society, including governments and citizens, must take the required steps to keep it crime-free."
  },
  {
    "objectID": "exploringdata.html",
    "href": "exploringdata.html",
    "title": "Exploring Data",
    "section": "",
    "text": "Exploring data is the visual depiction of data and information. Through the use of visual components such as charts, graphs, and maps, data visualization tools facilitate the identification and comprehension of trends, outliers, and patterns in data. In addition, it provides a good method for employees and business owners to deliver information to non-technical audiences without causing confusion.\nExploring data tools and technologies are vital in the realm of Big Data for analyzing enormous volumes of information and making data-driven decisions.\n\n\n\nQuickly comprehend information:\n\nOrganizations may examine vast volumes of data in simple, unified ways by using graphical representations of business information - and derive inferences from that information. Furthermore, because examining information in graphical format is substantially faster than analyzing information in spreadsheets, firms may handle problems or answer inquiries in a more timely manner.\n\nRecognize linkages and patterns:\n\nWhen presented visually, even large volumes of complex data begin to make sense; organizations may identify parameters that are highly connected. Some of the connections will be clear, while others will not. Identifying such linkages allows companies to focus on areas that are most likely to have an impact on their most important goals.\n\nIdentify emerging trends:\n\nUsing data visualization to detect patterns - both in the business and in the market - can provide firms with a competitive advantage and eventually affect the bottom line. It is simple to identify outliers that affect product quality or customer attrition and address concerns before they become major issues.\n\nShare the story with others:\n\nOnce a business has used visual analytics to find new insights, the next step is to share those insights with other people. In this step, it’s important to use charts, graphs, or other visually striking ways to show data because it’s interesting and gets the message across quickly.\n\n\n\n\n\n\nInaccurate or biased information.\nNot always does correlation imply causation.\nMessages of significance might be lost in translation.\n\n\n\n\nThe significance of data exploration is straightforward: it enables individuals to see, interact with, and better comprehend data. Regardless of their degree of knowledge, the correct visualization can bring everyone on the same page, whether the information is basic or complex.\n\n\n\n\nKnow the size and number of rows and columns of the data you want to display (the uniqueness of data values in a column).\nFigure out what you want to show and what kind of information you want to share.\nUnderstand your audience and how they process visual information.\nUtilize a visual that effectively and simply conveys the information to your audience.\n\n\n\n\nOne of the most difficult tasks for business users is determining which visual will best portray the information. SAS Visual Analytics utilizes intelligent autocharting to provide the optimal visual depending on the selected data. When exploring a new data collection for the first time, autocharts are particularly beneficial because they provide a fast overview of vast volumes of data. This data exploration feature is beneficial for even seasoned statisticians seeking to accelerate the analytics lifecycle process, since it eliminates the need for recurrent sampling to establish which data is suitable for each model.\n\n\n\n\n\nExploring Data with Python\n\n\nExploring Data with R\n\n\n\n\n\n\n\n        <br>\n        <center><a href=\"./images/VISUALISATION_WORDCLOUD_CRIME_TEXT_DATA.png\" target=\"new\">View </a></center>\n        <center><a href=\"https://github.com/anly501/anly-501-project-chaishekar/tree/main/code/exploring data/visualisation_crime_text_data.html\">View Code and Visualization</a></center>\n        <a href=\"./images/VISUALISATION_WORDCLOUD_CRIME_TEXT_DATA.png\" target=\"new\"> </a>\n        <center>The word cloud of Twitter data reveals the most common context in which #crime is used. Words like \"investigate\" and \"homicide\" are also common, giving some idea of the context in which individuals are using the hashtag. </center> \n\n        </td>\n        \n        <td width=\"50%\"><center><img alt=\"DIST\" height=\"500px\" src=\"./images/VISUALISATION_DIST_OFFENSE_CRIME_RECORD_DATA.png\" width=\"550px\"></center>\n        <br>\n        <center><a href=\"./images/VISUALISATION_DIST_OFFENSE_CRIME_RECORD_DATA.png\" target=\"new\">View </a></center>\n        <center><a href=\"https://github.com/anly501/anly-501-project-chaishekar/tree/main/code/exploring data/visulisation_crime_against_record_data.html\">View Code and Visualization</a></center>\n        <a href=\"./images/VISUALISATION_DIST_OFFENSE_CRIME_RECORD_DATA.png\" target=\"new\"> </a>\n        <center> There are numerous crimes occurring around us for a variety of reasons, and the above bar graph depicts the distribution of crime against people and society.</center> \n\n        </td>\n    </tr>\n    <!-- Add more rows here -->\n</tbody>\n\n\n’ )"
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "Support Vector Machines FOR TEXT DATA",
    "section": "",
    "text": "INTRODUCTION\nOn Twitter, a data collection effort for the hashtags “FBI” and “CRIMEUSA” is now being conducted. SVM can be utilized for classification or regression difficulties in this situation. It uses a technique known as the kernel trick to alter your data, and based on these transformations, it identifies the ideal border between the possible outputs.\nABOUT THE DATA\nThe text data is identical to the one used in Naive Bayes. The label text data is derived from the Twitter APIs for #FBI and #USACRIME. The hashtags #USACRIME and #CRIMEUSA are used interchangeably on Twitter to collect data about the use of hostages, as both serve the same purpose.\nIMPORT LIBRARIES\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report, accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nfrom sklearn.utils import resample\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom wordcloud import WordCloud, STOPWORDS\n\n\nIMPORT DATA\n\n\nCode\n# READ THE DATA\ncrimeusa_data = pd.read_csv('../data/raw-data/R_CRIMEUSA_TEXT_DATA.csv')\nusacrime_data = pd.read_csv('../data/raw-data/R_USACRIME_TEXT_DATA.csv')\nfbi_data = pd.read_csv('../data/raw-data/R_FBI_TEXT_DATA.csv')\ncleaned_tweet_data = pd.read_csv('../data/modified-data/cleaned_usacrime_text_data.csv')\ntweet_fbi_data = pd.read_csv('../data/modified-data/cleaned_fbi_text_data.csv')\n\n\nDATA CLEANING\nFBI and USA CRIME datasets are combined into one. To assess #crime, the data is purified by deleting all other columns from the dataset, leaving only the text column. Throughout this cleaning operation, Tokenization from the NLTK library and Countvectorizer from the scikit-learn library are both employed.\nThe text lemmatization is used for the subsequent step after the data has been cleaned. The technique of collecting together the various inflected forms of a word so that they can be studied as a single item is known as lemmatization. Lemmatization is similar to stemming in that it adds context to words. As a result, it connects words with similar meanings to a single word.\n\n\nCode\n#MERGE CRIME USA ANDD USA CRIME DATAFRAMES\ntweet_data_crimeusa = pd.concat([crimeusa_data, usacrime_data], axis=0)\ntweet_data_crimeusa.head()\n\n#ADD A COLUMN TO THE DATAFRAME\ntweet_data_crimeusa['label'] = 'USA CRIME'\nfbi_data['label'] = 'FBI'\n\n#REMOVE RETWEETS\ntweet_data_crimusa = tweet_data_crimeusa[~tweet_data_crimeusa['text'].str.contains('RT')]\nfbi_data = fbi_data[~fbi_data['text'].str.contains('RT')]\n\n#MERGE CLEAN TEXT COLUMN FROM CLEANED TWEET DATA WITH THE TWEET DATA\ntweet_data_crimeusa = pd.merge(tweet_data_crimeusa, cleaned_tweet_data, on='text', how='left')\nfbi_data = pd.merge(fbi_data, tweet_fbi_data, on='text', how='left')\n\n#DROP THE COLUMNS THAT ARE NOT NEEDED\ntweet_data_crimeusa = tweet_data_crimeusa.drop(['text','Tweet_tokenized','Tweet_without_stop','Tweet_stemmed'], axis=1)\nfbi_data = fbi_data.drop(['text','Tweet_tokenized','Tweet_without_stop','Tweet_stemmed'], axis=1)\n\n#REARRANGE THE COLUMNS\ntweet_data_crimeusa.insert(0, 'label', tweet_data_crimeusa.pop('label'))\ntweet_data_crimeusa.insert(1, 'clean text', tweet_data_crimeusa.pop('clean text'))\nfbi_data.insert(0, 'label', fbi_data.pop('label'))\nfbi_data.insert(1, 'clean text', fbi_data.pop('clean text'))\n\n#RENAME THE CLEAN TEXT COLUMN\ntweet_data_crimeusa = tweet_data_crimeusa.rename(columns={'clean text':'text'})\nfbi_data = fbi_data.rename(columns={'clean text':'text'})\n\n#MERGE FBI AND USA CRIME DATASETS\ntweet_data = pd.concat([tweet_data_crimeusa, fbi_data], axis=0)\n\n#REMOVE THE COLUMNS THAT ARE NOT NEEDED BEFORE EXPORTING THE DATA\n#tweet_data = tweet_data.drop(['Tweet_lemmatized'], axis=1)\n\n# HEAD OF THE DATA\ntweet_data.head()\n\n\n\n\n\n\n  \n    \n      \n      label\n      text\n      Unnamed: 0\n      favorited\n      favoriteCount\n      replyToSN\n      created\n      truncated\n      replyToSID\n      id\n      replyToUID\n      statusSource\n      screenName\n      retweetCount\n      isRetweet\n      retweeted\n      longitude\n      latitude\n      Tweet_lemmatized\n    \n  \n  \n    \n      0\n      USA CRIME\n      Onlyrockradio Donate tiorr internet radio supp...\n      1\n      False\n      1\n      Only_rock_radio\n      2022-10-10 21:56:24\n      True\n      1.579587e+18\n      1579591706663878656\n      4.913321e+09\n      <a href=\"https://mobile.twitter.com\" rel=\"nofo...\n      SceneCleaners\n      0\n      False\n      False\n      NaN\n      NaN\n      ['onlyrockradio', 'donate', 'tiorr', 'internet...\n    \n    \n      1\n      USA CRIME\n      Onlyrockradio Donate tiorr internet radio supp...\n      1\n      False\n      1\n      Only_rock_radio\n      2022-10-10 21:56:24\n      True\n      1.579587e+18\n      1579591706663878656\n      4.913321e+09\n      <a href=\"https://mobile.twitter.com\" rel=\"nofo...\n      SceneCleaners\n      0\n      False\n      False\n      NaN\n      NaN\n      ['onlyrockradio', 'donate', 'tiorr', 'internet...\n    \n    \n      2\n      USA CRIME\n      Enjoy exciting accounts of realism from the pe...\n      2\n      False\n      0\n      NaN\n      2022-10-10 21:09:40\n      True\n      NaN\n      1579579945655087104\n      NaN\n      <a href=\"https://mobile.twitter.com\" rel=\"nofo...\n      LaydenRobinson\n      0\n      False\n      False\n      NaN\n      NaN\n      ['enjoy', 'exciting', 'account', 'realism', 'p...\n    \n    \n      3\n      USA CRIME\n      Enjoy exciting accounts of realism from the pe...\n      2\n      False\n      0\n      NaN\n      2022-10-10 21:09:40\n      True\n      NaN\n      1579579945655087104\n      NaN\n      <a href=\"https://mobile.twitter.com\" rel=\"nofo...\n      LaydenRobinson\n      0\n      False\n      False\n      NaN\n      NaN\n      ['enjoy', 'exciting', 'account', 'realism', 'p...\n    \n    \n      4\n      USA CRIME\n      Metaverse Opinion amp Politics USA Trending Cr...\n      3\n      False\n      0\n      NaN\n      2022-10-10 20:21:00\n      True\n      NaN\n      1579567695825190917\n      NaN\n      <a href=\"https://mobile.twitter.com\" rel=\"nofo...\n      MetaversePosts\n      0\n      False\n      False\n      NaN\n      NaN\n      ['metaverse', 'opinion', 'amp', 'politics', 'u...\n    \n  \n\n\n\n\nLABEL COUNT\n\n\nCode\n#TEXT LEMMATIZED COLUMN\ndf =tweet_data[['label','Tweet_lemmatized']]\n\n#REMOVE PUNCTUATION FROM TEXT LEMMATIZED COLUMN\nfinal_tweets=[str(i).replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\") for i in df['Tweet_lemmatized']]\n\n#ADD THE FINAL TWEETS TO THE DATAFRAME\ndf['final_tweets']=final_tweets\n\n#remove text lemmatized column\n#REMOVE THE COLUMNS\ndf=df.drop('Tweet_lemmatized',axis=1)\n\n#COUNT THE NUMBER OF FBI AND CRIMEUSA DATA\nax = df['label'].value_counts().plot(kind='bar',\n                                    figsize=(14,8),\n                                    title=\"Number for labels\")\nax.set_xlabel(\"Labels\")\nax.set_ylabel(\"Frequency\")\n\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nThe bargraph tells us about the count of the labels i.e FBI and USA CRIME.\nSPLIT THE DATASET INTO TRAINING AND TESTING DATASETS\nBefore the data is split into training and testing data, it is scaled and labeled, with label encoding referring to turning the labels into a numeric form so that they can be machine-readable.\n\n\nCode\n#SEPARATE THE DATA INTO MAJORITY AND MINORITY CLASSES\ndf_majority = df[df.label=='FBI']\ndf_minority = df[df.label=='USA CRIME']\n\n\n#DOWN SAMPLE THE MAJORITY CLASS\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=len(df_minority),     # to match minority class\n                                 random_state=123) # reproducible results\n\n\n#COMBINE THE MINORITY AND DOWNSAMPLED MAJORITY CLASSES\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n\n\n#DISPLAY THE NUMBER OF DATA FOR EACH LABEL\nvalue_count = df_downsampled.label.value_counts()\nX=df_downsampled['final_tweets'].values\ny=df_downsampled['label'].values\n\n#LABEL ENCODING\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ny = labelencoder.fit_transform(y)\n\nimport random as rd\nMyCV_content = CountVectorizer(input='content', stop_words='english')\n\nMy_DTM2 = MyCV_content.fit_transform(X)\nColNames = MyCV_content.get_feature_names_out()  # Use get_feature_names_out()\nMy_DF_content = pd.DataFrame(My_DTM2.toarray(), columns=ColNames)\n\n#SPLIT THE DATA INTO TRAIN AND TEST\nMy_DF_content['LABEL'] = pd.DataFrame(y,columns=['LABEL'])\nrd.seed(1993)\nTrainDF, TestDF = train_test_split(My_DF_content, test_size=0.25)\nTrainLabels=TrainDF[\"LABEL\"]\nTestLabels=TestDF[\"LABEL\"]\n\nTrainDF = TrainDF.drop([\"LABEL\"], axis=1)\nTestDF = TestDF.drop([\"LABEL\"], axis=1)\n\n\nSUPPORT VECTOR MACHINE(SVM)\nA support vector machine, often known as an SVM, is a type of supervised machine learning model that solves problems involving two groups of categorization by employing classification techniques. When an SVM model is provided with sets of labeled training data for each category, the model is then able to classify newly encountered text.\nWhen compared to more recent algorithms such as neural networks, they have two primary advantages: increased speed and improved performance with a constrained quantity of data points (in the thousands). Because of this, the approach is ideally suited for solving problems involving the classification of text, which often involve having access to datasets containing no more than a few thousand annotated examples at most.\n\n\nCode\ndef svc_param_selection(X, y,k):\n    Cs = [ .01, 1, 5, 10]\n    param_grid = {'C': Cs}\n    grid_search = GridSearchCV(SVC(kernel=k), param_grid)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_\n\n\nSVM with LINEAR KERNELS\nWhen data is Linearly separable, meaning it can be partitioned along a straight line, a Linear Kernel is applied. It is one of the kernels that is utilized the most frequently. Its primary application occurs in situations where a given data set contains a significant number of features.\nTo be more specific, the svc() function can be utilized to fit a support vector classifier provided that the argument kernel = “linear” is utilized. This function implements the support vector classifier with a slightly modified version of the standard formulation. With the use of a cost argument, we are able to indicate the amount of money lost due to a margin violation. When the cost argument is low, the margins will be broad, and a significant number of support vectors will either be on the margin or will violate it. When the cost argument is significant, then the margins will be low, and there will be a small number of support vectors that are either on the margin or that violate the margin. Utilizing the svc() method allows us to tailor the support vector classifier to the value that has been supplied for the cost parameter, which is denoted by the letter ‘C’. For this model, the cost margin is taken as 0.5 and besides that we perform supervised machine learning (classification) on categorical data, we often use a confusion matrix to get the count of accurate and inaccurate predictions for different classes.\n\n\nCode\n#set seed\nnp.random.seed(26)\n#SVM MODEL WITH LINEAR KERNEL\n#GETTING THE BEST COST MARGIN\nsvc_param_selection(TrainDF, TrainLabels,\"linear\")\n#PREDICT THE TEST DATA\nSVM_Model=SVC(kernel='linear', C=0.5, probability=True)\nSVM_Model.fit(TrainDF, TrainLabels)\n\nPreds_SVM1 = SVM_Model.predict(TestDF)\nPred_Proba_SVM1 = SVM_Model.predict_proba(TestDF)\nSVM_matrix = confusion_matrix(TestLabels, Preds_SVM1)\nprint(metrics.classification_report(TestLabels, Preds_SVM1))\n\n#VISUALIZE THE CONFUSION MATRIX\nplt.figure(figsize=(10,10))\nlabels = ['CRIME USA', 'FBI']\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(TestLabels, Preds_SVM1), annot=True, fmt='g', ax=ax1)\n\n#LABELS, TITLE AND TICKS\nax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels')\nax1.set_title('Confusion Matrix')\nax1.xaxis.set_ticklabels(labels)\nax1.yaxis.set_ticklabels(labels)\nplt.show()\nplt.close()\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99       251\n           1       0.99      1.00      0.99       244\n\n    accuracy                           0.99       495\n   macro avg       0.99      0.99      0.99       495\nweighted avg       0.99      0.99      0.99       495\n\n\n\n\n\n\nINFERENCE:\nFrom the SVM model with Linear Kernel for the Cost margin of 0.5, the classification report says that the accuracy of the model is 99%. The precision of the model is 99%, the recall is 98% and the F1 Score is 99%. The model is not overfitting as the precision and recall values are the same. The model is not underfitting because the accuracy of the model is greater than 50%. The model is not biased because the precision and recall are the same.\nPOLYNOMIAL KERNEL with SVM\nPolynomial Kernel depicts the degree of similarity between vectors in the training set of data in a feature space over polynomials of the variables that were initially employed in the kernel.The polynomial kernel is a kernel function in machine learning that is commonly used with support vector machines (SVMs) and other kernelized models to represent the similarity of vectors (training samples) in a feature space over polynomials of the original variables and facilitate the learning of non-linear models.\nTo be more explicit, if the input kernel = “poly” is used with the svc() function, a support vector classifier can be fitted. By presenting the cost of a margin violation, we may give an approximate idea of the financial damage it causes. By calling the svc() function, we can adjust the support vector classifier to the value of the cost argument (denoted by the letter C). Since we are performing supervised machine learning (classification) on categorical data, we frequently use a confusion matrix to determine the count of accurate and inaccurate predictions for distinct classes; in this model, we set the cost margin to 0.5.\n\n\nCode\n#set seed\nnp.random.seed(2)\n#POLYNOMIAL KERNEL MODEL\n#GETTING THE BEST COST MARGIN\nsvc_param_selection(TrainDF, TrainLabels,\"poly\")\n\n#POLY KERNAL, C = 0.5\nSVM_Model2=SVC(kernel='poly', C=0.5,probability=True)\nSVM_Model2.fit(TrainDF, TrainLabels)\n\nPreds_SVM2 = SVM_Model2.predict(TestDF)\nPred_Proba_SVM2 = SVM_Model2.predict_proba(TestDF)\nSVM_matrix = confusion_matrix(TestLabels, Preds_SVM2)\nprint(metrics.classification_report(TestLabels, Preds_SVM2))\n\n#VISCALIZE THE CONFUSION MATRIX\nplt.figure(figsize=(10,10))\nlabels = ['CRIME USA', 'FBI']\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(TestLabels, Preds_SVM2), annot=True, fmt='g', ax=ax1)\n\n#LABELS, TITLE AND TICKS\nax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels')\nax1.set_title('Confusion Matrix')\nax1.xaxis.set_ticklabels(labels)\nax1.yaxis.set_ticklabels(labels)\nplt.show()\nplt.close()\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       251\n           1       1.00      1.00      1.00       244\n\n    accuracy                           1.00       495\n   macro avg       1.00      1.00      1.00       495\nweighted avg       1.00      1.00      1.00       495\n\n\n\n\n\n\nINFERENCE:\nFrom the SVM model with Polynomial Kernel for the Cost margin of 0.5, the classification report says that the accuracy of the model is 96%. The precision of the model is 97%, the recall is 96% and the F1 Score is 96%. In this model precision is greater than accuaracy which tells us that precision is independent of accurary. The model is not underfitting because the accuracy of the model is greater than 50%. The model is not imbalanced because the number of data for each label is the same.\nSVM with RBF KERNELS\nRBF kernels are the most generalized kind of kernelization. They are also one of the most often used kernels because of their resemblance to the Gaussian distribution, which is one of the most common distributions. It is a kernel that can be used for a variety of applications and is utilized in circumstances in which there is no prior information about the data. Specifically, it is used in cases where there is no prior knowledge about the data.\nTo be more explicit, if the input kernel = “rbf” is used with the svc() function, a support vector classifier can be fitted. By presenting the cost of a margin violation, we may give an approximate idea of the financial damage it causes. By calling the svc() function, we can adjust the support vector classifier to the value of the cost argument (denoted by the letter C). Since we are performing supervised machine learning (classification) on categorical data, we frequently use a confusion matrix to determine the count of accurate and inaccurate predictions for distinct classes; in this model, we set the cost margin to 0.5.\n\n\nCode\n#set seed\nnp.random.seed(2)\n#RBF MODEL\n#GETTING THE BEST COST MARGIN\nsvc_param_selection(TrainDF, TrainLabels,\"rbf\")\n\n# RBF Kernal - C = 0.5\nSVM_Model3=SVC(kernel='rbf', C=0.5,probability=True)\nSVM_Model3.fit(TrainDF, TrainLabels)\n\nPreds_SVM3 = SVM_Model3.predict(TestDF)\nPred_Proba_SVM3 = SVM_Model3.predict_proba(TestDF)\nSVM_matrix3 = confusion_matrix(TestLabels, Preds_SVM3)\nprint(metrics.classification_report(TestLabels, Preds_SVM3))\n\n#VISUALIZE THE CONFUSION MATRIX\nplt.figure(figsize=(10,10))\nlabels = ['CRIME USA', 'FBI']\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(TestLabels, Preds_SVM3), annot=True, fmt='g', ax=ax1)\n\n#LABELS, TITLE AND TICKS\nax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels')\nax1.set_title('Confusion Matrix')\nax1.xaxis.set_ticklabels(labels) \nax1.yaxis.set_ticklabels(labels)\nplt.show()\nplt.close()\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.99      0.98       251\n           1       0.99      0.96      0.98       244\n\n    accuracy                           0.98       495\n   macro avg       0.98      0.98      0.98       495\nweighted avg       0.98      0.98      0.98       495\n\n\n\n\n\n\nINFERENCE:\nFrom the SVM model with RBF Kernel for the Cost margin of 0.5, the classification report says that the accuracy of the model is 99%. The precision of the model is 99%, the recall is 99% and the F1 Score is 99%. The model is not overfitting as the precision and recall values are the same. The model is not underfitting because the accuracy of the model is greater than 50%. The model is not biased because the precision and recall are the same.\nCONCLUSION\nThe SVM model was used to classify tweets that were generated from Twitter into the respective hashtag classes (FBI and USA CRIME) of other tweets. When it comes to determining the category, the models have an accuracy ranging from 96% to 99%, which is considered to be rather high. The model is able to make predictions about the tweets or to categorize them into different groups thanks to the collection of terms. The linear model and RBF Kernel model are the most appropriate representations of these data since they account for 99% of the variance."
  }
]